<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>WanLi6</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="WanLi6">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="WanLi6">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Wan Li">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="WanLi6" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">WanLi6</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">My Personal Study Blogs</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-SUGRL" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/SUGRL/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T12:06:58.555Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/SUGRL/">SUGRL</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Simple-Unsupervised-Graph-Representation-Learning-简单的无监督图表示学习"><a href="#Simple-Unsupervised-Graph-Representation-Learning-简单的无监督图表示学习" class="headerlink" title="Simple Unsupervised Graph Representation Learning(简单的无监督图表示学习)"></a>Simple Unsupervised Graph Representation Learning(简单的无监督图表示学习)</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li><p>本文提出一种简单的<strong>无监督图表示学习</strong>方法以实现有效且高效的对比学习。</p>
</li>
<li><p>具体来说，本文提出的<strong>多重损失</strong>探索了<strong>结构信息</strong>和<strong>邻域信息</strong>之间的互补信息以<strong>增大类间差异</strong>，同时增加<strong>上限损失</strong>以实现正嵌入和锚嵌入之间的有限距离以<strong>减少类内差异</strong>。</p>
</li>
<li><p>此外，本文的方法从以前的图对比学习方法中删除了广泛使用的数据增强和鉴别器，同时可以输出低维嵌入，产生一个有效的模型。</p>
</li>
</ul>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ol>
<li>首先，为了保证算法的有效性，提出了联合考虑结构信息和邻域信息来挖掘它们的互补信息，目的是扩大类间变异，以及设计一个上限损失来实现小的类内变异</li>
<li>其次，为了提高效率，我们将数据增强和鉴别器从对比学习中删除。这使得本文的方法很容易在大规模数据集上实现可扩展性。</li>
<li>最后，通过对8个公共基准数据集的综合实证研究，与11种比较方法进行了比较，验证了本文方法在节点分类方面的有效性和效率</li>
</ol>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="https://img-blog.csdnimg.cn/e229176647f8420fa5765d9d4ab430e3.png" alt="The flowchart"></p>
<p>SUGRL侧重于MI最大化。在文献中，考虑到MI最大化的计算成本，SUGRL中的MI最大化被转移到对比学习，其中涉及锚嵌入，正嵌入和负嵌入，以及对比损失的定义。</p>
<h3 id="Anchor-and-Negative-Embedding-Generation"><a href="#Anchor-and-Negative-Embedding-Generation" class="headerlink" title="Anchor and Negative Embedding Generation"></a>Anchor and Negative Embedding Generation</h3><p>现有的工作通常将节点表示或图形摘要视为锚点。</p>
<p>本文对于输入特征 $X$ 使用一个<strong>MLP</strong>以生成带有语义信息的 <strong>anchor embedding</strong></p>
<p>$$\mathbf{X}^{(l+1)}&#x3D;Dropout\left(\sigma\left(\mathbf{X}^{(l)}\mathbf{W}^{(l)}\right)\right)$$</p>
<p>$$\mathbf{H}&#x3D;\mathbf{X}^{(l+1)}\mathbf{W}^{(l+1)}$$</p>
<ul>
<li>$X^{(0)} &#x3D; X$</li>
<li>$\sigma$ 是激活函数</li>
<li>$\mathbf{W}^{(l)}$ 是第 $l$ 层的权重</li>
</ul>
<p>对于负嵌入的生成，流行的方法（例如，DGI、GIC和MVGRL）是从原始图中获得损坏的图，然后用GCN对其进行处理。相比之下，本文直接对锚嵌入进行<strong>行混洗</strong>得到负嵌入，进一步减少了训练时间。</p>
<p>$$\mathbf{H}^-&#x3D;\textit{Shuffle }([\mathbf{h}_1,\mathbf{h}_2,\ldots,\mathbf{h}_N])$$</p>
<p>总之，本文提出的方法通过去除GCN，减少了计算成本的同时保持了有效性。</p>
<h3 id="Positive-Embedding-Generation"><a href="#Positive-Embedding-Generation" class="headerlink" title="Positive Embedding Generation"></a>Positive Embedding Generation</h3><p>现存的工作大多将结构信息视作正嵌入。此外，以往的工作通常采用数据增强来获得不同的信息，以进行有效的对比学习，</p>
<p>相比之下，在本文中我们提出通过生成两种正嵌入来获得不同的信息，即，<strong>结构嵌入和邻域嵌入</strong>。具体来说，我们采用<strong>GCN</strong>和<strong>邻居采样</strong>方法来生成它们。</p>
<h4 id="Structural-information"><a href="#Structural-information" class="headerlink" title="Structural information"></a>Structural information</h4><p>为了得到图的结构信息，我们使用广泛使用的GCN作为基本编码器。</p>
<p>$$\mathbf{H}^{+^{(l+1)}}&#x3D;\sigma\left(\widehat{\mathbf{A}}\mathbf{H}^{+^{(l)}}\mathbf{W}^{(l)}\right)$$</p>
<ul>
<li>$\mathbf{H}^{+(0)}&#x3D;\mathbf{X}$ 且 $\mathbf{H}^{+(l)}$意味着第l层的特征</li>
<li>$\widehat{\mathbf{A}}&#x3D;\hat{\mathbf{D}}^{-1&#x2F;2}\tilde{\mathbf{A}}\hat{\mathbf{D}}^{-1&#x2F;2}\in\mathbb{R}^{N\times N}$ 是对称规范化邻接矩阵</li>
<li>$\hat{\mathbf{D}}\in\mathbb{R}^{N\times N}$ 是 $\tilde{\mathbf{A}}&#x3D;{\mathbf{A}}+{\mathbf{I}}_N$ 的度矩阵。${\mathbf{I}}_N$ 是单位矩阵</li>
</ul>
<p>值得注意的是，本文的方法直接共享了MLP和GCN编码器的权重（例如 $\mathbf{W}^{(l)}$），以进一步减少时间消耗。</p>
<h4 id="Neighbor-information"><a href="#Neighbor-information" class="headerlink" title="Neighbor information"></a>Neighbor information</h4><p>为了获得带有邻域信息的正嵌入，本文首先存储所有节点的邻居嵌入索引，然后进行采样，接着计算样本的平均值。这样可以高效的获取节点的邻域信息。</p>
<p>$$\widetilde{\mathbf{h}}<em>i^+&#x3D;\frac1m\sum</em>{j&#x3D;1}^m\left{\mathbf{h}_j\mid v_j\in\mathcal{N}_i\right}$$</p>
<p>总之，结构嵌入和邻居嵌入分别关注所有邻居和部分邻居。也就是说，结构嵌入代表一般表示，而邻居嵌入是特定表示。因此，他们从不同的角度解释样本，将它们一起考虑，以尽可能获得它们的互补信息。</p>
<h3 id="Multiplet-Loss"><a href="#Multiplet-Loss" class="headerlink" title="Multiplet Loss"></a>Multiplet Loss</h3><p>给定锚嵌入、正嵌入和负嵌入，对比学习旨在进行正对（即，锚和正嵌入）接近，同时保持负对（即，锚和负嵌入）相距很远。许多对比学习方法（例如，DGI、GMI、MVGRL和GIC）设计鉴别器（例如，双线性层）来区分正对和负对，但是鉴别器是耗时的。此外，减少泛化误差对于UGRL也很重要，因为训练过程中的小泛化误差可能会提高对比学习的泛化能力。此外，无论是减小类内变异还是扩大类间变异，都已被证明是减小泛化误差的有效方法</p>
<p>在SUGRL中，我们考虑的三重损失为基础，并设计一个上限损失，以从本文的方法中移除鉴别器（为了提高效率），并减少类内的差异，以及扩大类间的差异（为了提高有效性）。具体地，相对于每个采样的三重态损失可以公式化为：</p>
<p>$$\alpha+d\left(\mathbf{h},\mathbf{h}^+\right)&lt;d\left(\mathbf{h},\mathbf{h}^-\right)$$</p>
<ul>
<li>$d(\cdot)$ 是一个相似度度量（例如：$\ell_2$范数距离）</li>
<li>$\alpha$ 是一个非负值，以确保正嵌入和负嵌入之间的“安全”距离。</li>
</ul>
<p>通过对所有负嵌入的损失求和，上式可以扩展到</p>
<p>$$\mathcal{L}<em>{triplet}&#x3D;\frac1k\sum</em>{i&#x3D;1}^k\left{d(\mathbf{h},\mathbf{h}^+\right)^2-d\left(\mathbf{h},\mathbf{h}<em>i^-\right)^2+\alpha}</em>+$$</p>
<ul>
<li>$${\cdot}_{+}&#x3D;\max{\cdot,0}$$</li>
<li>$k$ 是负样本的个数</li>
</ul>
<p>为了增加类间的变异，我们应该把负对推得远离正对。为了实现这一点，我们采用了在上一节中定义的两种正嵌入上的三重损失，以得到：</p>
<p>$$\mathcal{L}<em>S&#x3D;\frac1k\sum</em>{i&#x3D;1}^k\left{d\left(\mathbf{h},\mathbf{h}^+\right)^2-d\left(\mathbf{h},\mathbf{h}<em>i^-\right)^2+\alpha\right}</em>+$$</p>
<p>$$\mathcal{L}<em>N&#x3D;\frac{1}{k}\sum</em>{j&#x3D;1}^k\left{d\left(\mathbf{h},\widetilde{\mathbf{h}}^+\right)^2-d\left(\mathbf{h},\mathbf{h}<em>j^-\right)^2+\alpha\right}</em>+$$</p>
<p>根据上一节的内容，结构嵌入（$\mathrm{h^{+}}$）与邻域嵌入不同（$\mathrm{\tilde{h}^{+}}$）。</p>
<p>根据两种类型的正嵌入与锚嵌入的距离不同，可以分为两种情况</p>
<p>Case 1: $$d\left(\mathbf{h},\mathbf{h}^{+}\right)^{2}\geq d(\mathbf{h},\widetilde{\mathbf{h}}^{+})^{2}$$</p>
<p>Case 2: $$d\left(\mathbf{h},\mathbf{h}^{+}\right)^{2}&lt;d(\mathbf{h},\widetilde{\mathbf{h}}^{+})^{2}$$</p>
<p>如果是Case 1，在$\mathcal{L}_S$是0的情况是，$\mathcal{L}_N$一定是非0的。在这种情况下$\mathcal{L}_N$仍然是有效的，而$\mathcal{L}_S$是无效的。结果，负嵌入将继续远离锚嵌入，类间差异被放大。与Case 1类似，Case 2也可以扩大类间差异。</p>
<p>基于上述分析，Case 1或Case 2都可以扩大类间差异。特别是，如果其中一个是无效的，另一个仍然有效，以进一步扩大类间差异。因此，$\mathcal{L}_S$ 和 $\mathcal{L}_N$ 可以从结构嵌入和相邻嵌入中获得互补信息，从而能够放大类间的变异。</p>
<p>$\mathcal{L}_{triplet}$要求 $d\left(\mathbf{h},\mathbf{h}^{+}\right)^{2}$ 和 $d\left(\mathbf{h},\mathbf{h}<em>i^-\right)^2$ 之间的距离应该大于$\alpha$，但它忽略了锚嵌入和正嵌入之间的距离。如果锚嵌入和正嵌入之间的距离大，则$\mathcal{L}</em>{triplet}$中的{·}+项也可以是非零的。然而，在这种情况下，类内变化可能很大，不利于减少泛化误差。</p>
<p>为了解决这个问题，本文通过以下目标函数对于负对和正对研究了一个上限（$\alpha + \beta$）：</p>
<p>$$\alpha+d\left(\mathbf{h},\mathbf{h}^+\right)&lt;d\left(\mathbf{h},\mathbf{h}^-\right)&lt;d\left(\mathbf{h},\mathbf{h}^+\right)+\alpha+\beta $$</p>
<ul>
<li>$\beta$是一个非负微调参数</li>
<li>上界 $\alpha + \beta$ 保证了负嵌入和锚嵌入之间的距离是有限的，所以正嵌入和锚嵌入之间的距离也是有限的（$$\alpha+d\left(\mathbf{h},\mathbf{h}^+\right)&lt;d\left(\mathbf{h},\mathbf{h}^-\right)$$）</li>
</ul>
<p>最终减少了类内差异。在对所有负嵌入的损失求和之后，所提出的减小类内变化的上限损失定义如下：</p>
<p>$$\mathcal{L}<em>U&#x3D;-\frac{1}{k}\sum</em>{i&#x3D;1}^k\left{d\left(\mathbf{h},\mathbf{h}^+\right)^2-d\left(\mathbf{h},\mathbf{h}<em>i^-\right)^2+\alpha+\beta\right}</em>-$$</p>
<ul>
<li>$${\cdot}_{-}&#x3D;\min{\cdot,0}$$</li>
</ul>
<p>值得注意的是，上界没有被应用于neighbor information。这是由于</p>
<ol>
<li>每种信息都能达到相似的结果</li>
<li>在实验中同时使用它们并不能显着提高模型性能</li>
</ol>
<p>最终，融合三重损失以及上界损失，本文提出的多重损失函数如下：</p>
<p>$$\mathcal{L}&#x3D;\omega_1\mathcal{L}_S+\omega_2\mathcal{L}_N+\mathcal{L}_U$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/SUGRL/" data-id="clnn4xje10006w0u25e1xapc9" data-title="SUGRL" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-SGNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/SGNN/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T12:06:58.553Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/SGNN/">SGNN学习</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="基于图嵌入和图神经网络的社交网络影响力最大化方法"><a href="#基于图嵌入和图神经网络的社交网络影响力最大化方法" class="headerlink" title="基于图嵌入和图神经网络的社交网络影响力最大化方法"></a>基于图嵌入和图神经网络的社交网络影响力最大化方法</h1><h2 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h2><ul>
<li>提出了一种新的使用<strong>struc2vec图嵌入</strong>和<strong>基于图神经网络的回归器</strong>的影响力最大化方法</li>
<li>将影响力最大化问题看作一个<strong>伪回归任务</strong>使深度学习和机器学习的方法可以应用到这个任务中</li>
<li>使用LSTM单元作为GNN中的邻域聚合器</li>
</ul>
<h2 id="方法概述："><a href="#方法概述：" class="headerlink" title="方法概述："></a>方法概述：</h2><ul>
<li><p>影响力最大化问题中，一个很重要的部分是根据节点可能的影响力对节点进行排序。</p>
</li>
<li><p>这个可能的影响力形成了一组<strong>连续</strong>的值。因此，将影响最大化的问题解释为<strong>基于特定节点特征预测形成伪回归活动的连续值集的任务</strong>。</p>
</li>
<li><p>节点特征应保持网络中节点的<strong>结构特点及其拓扑特征</strong>。为了提取和处理这些节点特征，利用<strong>struc2vec节点嵌入</strong>方法来为网络的每个节点生成合适维度的特征向量。这简化了要在网络上执行的各种机器学习和深度学习任务的适用性。</p>
</li>
<li><p>生成的节点嵌入由GNN架构进一步处理。然后将这些处理后的嵌入传递到回归器上，以预测网络中的节点所实现的最终影响力扩散。</p>
</li>
<li><p>算法的<strong>基本功能</strong>是在<strong>训练网络</strong>上训练所提出的基于GNN的模型以获得模型参数，然后在<strong>目标网络</strong>上使用该训练模型来执行影响最大化。</p>
</li>
<li><p>通过计算信息扩散模型下训练网络的节点的个体影响来生成训练模型所需的标签。</p>
</li>
<li><p><strong>四个阶段：</strong></p>
<ul>
<li>生成标签</li>
<li>通过嵌入生成特征</li>
<li>通过GNN处理特征</li>
<li>使用回归器预测影响力传播</li>
</ul>
</li>
</ul>
<h2 id="1-标签生成"><a href="#1-标签生成" class="headerlink" title="1 标签生成"></a>1 标签生成</h2><p>在本研究中，将影响最大化问题解释为伪回归问题。然而，对于任何回归任务，我们都需要一组定义良好且连续的标签。</p>
<p>沿着这些路线，我们还要求标签回归网络特征以进行训练。利用Barabasi-Albert（BA）合成网络的几个变体作为训练网络，训练我们的模型以进行影响最大化任务。</p>
<p>沿着伪回归任务的想法，在SIR信息扩散模型下计算每个节点的影响力。该计算的影响力用于训练回归任务的SGNN模型的标签。</p>
<p>表示如下：</p>
<p>$$label^G:&#x3D;IDM(G(V,E))$$</p>
<h2 id="2-使用struc2vec嵌入生成特征"><a href="#2-使用struc2vec嵌入生成特征" class="headerlink" title="2 使用struc2vec嵌入生成特征"></a>2 使用struc2vec嵌入生成特征</h2><p>大多数现实生活中的网络，如在线社交网络，都在不断发展，规模庞大，通常难以处理和分析。某些网络还关联特定的节点属性，但并非所有网络都是如此。为了解决这个困难，我们使用节点嵌入技术为每个网络生成节点属性。</p>
<p>因此，我们的目标是提供一个通用的影响最大化框架，利用网络结构来生成网络中的节点的功能。作为这项工作的一部分，我们采用了基于struct2vec节点嵌入的方法来为网络中的每个节点生成低维向量。</p>
<p>$$S&#x3D;struc2vec(G,d)$$</p>
<p>节点v的嵌入表示为：</p>
<p>$$s_v&#x3D;S[v],v\in V$$</p>
<h2 id="3-使用GNN处理特征"><a href="#3-使用GNN处理特征" class="headerlink" title="3 使用GNN处理特征"></a>3 使用GNN处理特征</h2><p>基于图神经网络（GNN）的特征处理。使用图的节点之间的<strong>消息传递</strong>和<strong>邻域聚合</strong>来捕获图信息。它有助于表示来自具有任意深度的节点的邻域的信息，定义从该节点到形成邻域的跳数。</p>
<p>通过struct2vec捕获的网络细节通过GNN架构进一步增强和增强。这有助于更好地说明网络节点的结构细节。GNN中的聚合器函数可以是参数的，也可以是非参数的。</p>
<p>在我们的研究中，我们考虑了一个参数聚合函数，因为它通过不断更新学习参数的值来更深刻地捕捉网络的复杂结构，以获得更好的结果。GNN生成表示网络中每个节点v的特征的最终状态向量，如下所示。</p>
<p>$$h_{v}&#x3D;f(s_{v},h_{\text{ne}[v]},s_{\text{ne}[v]}),v\in V$$</p>
<ul>
<li>$h_v$是最终的状态向量</li>
<li>$s_v$是节点v生成的嵌入向量</li>
<li>$h_{ne[v]}$是节点v邻居节点的最终状态向量</li>
<li>$s_{ne[v]}$是节点v邻居节点的嵌入向量</li>
<li>$f$是邻域聚合函数，此处使用LSTM</li>
</ul>
<p>逐层更新公式为：</p>
<p>$$H^{i+1}:&#x3D;F(H^i,S)$$</p>
<h2 id="4-使用一个回归器进行最终的影响力传播预测"><a href="#4-使用一个回归器进行最终的影响力传播预测" class="headerlink" title="4 使用一个回归器进行最终的影响力传播预测"></a>4 使用一个回归器进行最终的影响力传播预测</h2><p>通常，回归器用于输入数据点的一组特征，并在优化损失函数的同时生成一组连续值作为输出。</p>
<p>回归器是所提出的SGNN架构中<strong>预测网络中可能的影响力大小</strong>的部分。在先前步骤由GNN产生的最终特征向量被送到回归器中。回归器使用<strong>均方误差</strong>作为损失函数。</p>
<p>一旦被训练，整个模型就被用于对目标网络中的节点的可能影响力进行预测。回归器的工作是将影响力预测作为一组连续值。然后根据节点的预测影响对节点进行排列</p>
<p>$$Inf_v&#x3D;o(h_v,s_v,\beta)$$</p>
<p>$$loss&#x3D;\frac{\Sigma_{v&#x3D;1}^{|V|}(Calc_v-Inf_v)^2}2$$</p>
<h4 id="训练和测试模型的过程如下"><a href="#训练和测试模型的过程如下" class="headerlink" title="训练和测试模型的过程如下"></a>训练和测试模型的过程如下</h4><p>$$trained_SGNN :&#x3D; SGNN(G^{train},S^{G^{train}},label^{G^{train}},f,o)$$</p>
<p>$$Predicted_Influence :&#x3D; trained SGNN(G^{test},S^{G^{test}})$$</p>
<p>$$Spread :&#x3D; trained SGNN(G^{test},S^{G^{test}},k)$$</p>
<ul>
<li>使用带标签的训练网络对模型进行训练</li>
<li>使用训练好的模型在测试网络计算结点的影响力</li>
<li>计算前k个节点的影响力传播</li>
</ul>
<h2 id="参数分析"><a href="#参数分析" class="headerlink" title="参数分析"></a>参数分析</h2><p>因为我们通过在一个网络上训练SGNN并在完全不同的网络上进行影响力预测。因此，我们执行参数分析以确定最佳训练网络。为此，我们在BA模型下创建了两组网络，分别用于训练和测试的$D^{Train}$和$D^{Test}$。通过在不同的训练网络上训练SGNN，SGNN的性能评估如下</p>
<p>$$trained_SGNN:&#x3D;SGNN(D^{train}[i],S^{D^{train}[i]},label^{D^{train}[i]},f,o),i\in range(1,len(D^{train})),$$</p>
<p>$$F(t_c)[{i}][{j}]:&#x3D;\sum_{k&#x3D;10\wedge k+&#x3D;5}^{k&#x3D;50}{\textit{trained SGNN}(D^{\text{test}} [ { j }],S^{\text{D}^{\text{test}} [  { j }]},k),j\in\text{range}(1,\text{len}(D^{\text{test}} ) ) }$$</p>
<ul>
<li>$F(t_c)[{i}][{j}]$代表最终的感染范围或者影响力扩散范围</li>
</ul>
<p>根据如下公式选择训练网络：</p>
<p>$$Opt_Train:&#x3D;argmax\left{\sum_{j&#x3D;1}^{j&#x3D;len(D^{test})}\frac{F(t_c)[i][j]}{\sum_{q&#x3D;1}^{q&#x3D;len(D^{train})}F(t_c)[q][j]},i\in D^{train}\right}$$</p>
<h2 id="总结和思考："><a href="#总结和思考：" class="headerlink" title="总结和思考："></a>总结和思考：</h2><h3 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h3><p>本文使用的方法核心为将<strong>影响力最大化问题转化为一个伪回归问题</strong></p>
<ul>
<li><p>回归问题中的标签：<strong>使用SIR模拟获取节点的影响力大小作为标签</strong></p>
</li>
<li><p>对于特征的处理:</p>
<ul>
<li><p>首先使用struc2vec生成节点的嵌入作为<strong>节点特征</strong></p>
<ul>
<li><p>struct2vec使用层次结构来度量不同尺度的节点相似性，并构造多层图来编码结构相似性并生成节点的结构上下文。最终通过结构上下文进行word2vec嵌入</p>
</li>
<li><p>由于struc2vec使用的层次结构信息,因此生成的嵌入可以很好的<strong>捕捉节点周围的拓扑结构信息</strong></p>
</li>
</ul>
</li>
<li><p>使用GNN处理上边生成的节点特征</p>
<ul>
<li>使用一个聚合器聚合 邻居的<em>最终状态向量</em>$h_{ne}$ , 邻居的嵌入向量$s_{ne}$, 自身的嵌入向量$h$ , 聚合后更新最终状态向量</li>
<li>其中h的初始化文中没有提及, 此处使用了嵌入向量初始化最终状态向量</li>
<li>这一步使用GNN处理, 可以<strong>聚合邻居节点的信息</strong>, 通过迭代在邻域上的聚合<strong>捕捉更加复杂的网络结构</strong>, 使用GNN也可以<strong>提高模型的泛化性</strong>, 提升在其他网络上的表现.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h3><h4 id="1-对影响力重叠问题做处理"><a href="#1-对影响力重叠问题做处理" class="headerlink" title="1. 对影响力重叠问题做处理"></a>1. 对影响力重叠问题做处理</h4><p>在该论文的方法中，最终寻找的影响力最大的种子节点集可能会出现聚集的现象，导致影响力重叠问题。</p>
<p>可以在最后对候选节点进行筛选（距离过近的删除？）（需要进一步考虑如何筛选）</p>
<p>或者在前期嵌入阶段采取一些办法避免出现影响力重叠</p>
<h4 id="2-关于图中节点的属性补全？"><a href="#2-关于图中节点的属性补全？" class="headerlink" title="2. 关于图中节点的属性补全？"></a>2. 关于图中节点的属性补全？</h4><p>本文中讨论的图均不考虑节点属性，然而现在越来越多的数据集中节点带有属性。</p>
<p>是否可以对不同的数据集做不同的处理：</p>
<p>对不带属性的图，先进行属性补全（使用其他方法获取嵌入作为特征或者使用一些中心性指标作为初始特征）</p>
<p>然后对于带属性的图，通过GNN聚合特征，判断影响力大小</p>
<h4 id="3-对嵌入方法（生成特征）进行改进"><a href="#3-对嵌入方法（生成特征）进行改进" class="headerlink" title="3. 对嵌入方法（生成特征）进行改进?"></a>3. 对嵌入方法（生成特征）进行改进?</h4><p>本文使用struc2vec获取嵌入向量。可以考虑使用其他更有效的嵌入方法</p>
<p>比如：</p>
<p><del>CARE : community-aware random walk for network embedding</del></p>
<p><del>这种方法的优点（为什么选择它）：</del></p>
<p><del>降低花费的计算时间</del></p>
<p><del>邻域不仅包含一阶邻居的信息，而且可以包含不直接相连的节点（具有一阶和二阶邻域不包含的同质性）</del></p>
<p>对于影响力传播，一阶和二阶邻居应该是是比较重要的。</p>
<p>考虑LINE和SDNE（一阶和二阶方法）</p>
<p>或者调整struc2vec的层数</p>
<h4 id="4-该方法是否能扩展到异质图上-（暂时不考虑）"><a href="#4-该方法是否能扩展到异质图上-（暂时不考虑）" class="headerlink" title="4. 该方法是否能扩展到异质图上?（暂时不考虑）"></a>4. <del>该方法是否能扩展到异质图上?</del>（暂时不考虑）</h4><p><strong>问题一</strong> : 生成特征可以用哪些方法? </p>
<p>metapath2vec等异质图嵌入方法</p>
<p><strong>问题二</strong> : 处理特征时需要有哪些改变?</p>
<p>由于本文的GNN只涉及消息传递与聚合, 不需要过多改变就可以使用在异质图上.</p>
<p>但是,也可以对GNN进行一些完善, 比如加上 激活函数等</p>
<p><strong>问题三</strong> : 本文最后得到的模型是一个泛化的通用模型, 在异质图上可以得到吗?</p>
<p>本文中使用了很多基于BA模型生成的数据集进行训练和测试, 最后通过比较选择了最好的参数. 然后用于其他数据集.</p>
<p>对于异质图, 对于不同节点类型数目的网络, 模型可能泛化性很差. </p>
<p>但是可以考虑如下情况, 在同一类异质图上(比如, 学术网络), 通过不同规模的学术网络数据集进行训练, 选择参数, 最后在其他同类数据集上直接使用.</p>
<p><strong>问题四</strong> : 异质图上边的类型不同, 信息传播模型是否需要改变?</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/SGNN/" data-id="clnn4xje00005w0u2evq2h3mj" data-title="SGNN学习" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-PathNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/PathNet/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T12:06:58.551Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/PathNet/">PathNet学习</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="PathNet"><a href="#PathNet" class="headerlink" title="PathNet"></a>PathNet</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>GNN聚合函数的同质性假设限制了他们在异配图上的学习能力。</li>
<li>本文揭示了图中路径级别模式可以明确的反应丰富的语义和结构信息。</li>
<li>因此提出了一个新的<strong>结构感知路径聚合</strong>图神经网络（PathNet），以泛化GNN到同配和异配图。</li>
<li>首先，本文介绍了一个最大熵路径采样器，它可以帮助我们对包含结构上下文的多条路径进行采样。</li>
<li>然后，本文引入了一个结构感知的递归单元组成的顺序保持和距离感知组件学习的语义信息的邻域。</li>
<li>最后，对路径编码后不同路径对目标节点的偏好进行建模。</li>
</ul>
<h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><p>如何得到多个路径并编码正确的路径以获取足够的信息是极具挑战性的。</p>
<ol>
<li>如何定义一个适当的采样器，可以避免过度膨胀的问题，同时保留有意义的结构信息</li>
<li>如何设计一个次序保持的聚合器。现有的GNN聚合器对Path中的order不敏感</li>
<li>如何捕捉路径对于不同节点的偏好</li>
</ol>
<p>本文中，提出了一种 <strong>结构感知路径聚合图神经网络</strong> 来解决这些问题。</p>
<ol>
<li>为了解决过度膨胀问题并保持结构，引入一个<strong>最大熵路径采样器</strong></li>
<li>为了在保留高阶邻域上下文的同时对路径编码，引入一种<strong>结构感知路径编码器</strong>：具有以下优点<ol>
<li>通过循环机制保留顺序</li>
<li>通过利用到目标结点的结构来捕获上下文</li>
</ol>
</li>
<li>在路径编码之后，提出一种<strong>路径注意力机制</strong>模拟具有不同邻域同质性水平的节点的路径偏好。</li>
</ol>
<h2 id="PathNet-1"><a href="#PathNet-1" class="headerlink" title="PathNet"></a>PathNet</h2><p><img src="https://img-blog.csdnimg.cn/8dd9039b63ba469eaee5694335286fac.png" alt="PathNet"></p>
<p><strong>同配性和异配性</strong>：同质性的概念源于节点与其邻居具有相同类的倾向。</p>
<p>同质性程度可以通过同质性边比例来定量描述：$h(\mathcal{G})&#x3D;\frac{|{(u,v){:}(u,v){\in}\mathcal{E}\wedge y_{u}&#x3D;y_{v}}|}{|\mathcal{E}|}$</p>
<p>路径在表示图的复杂语义信息方面具有很大的潜力。</p>
<h3 id="Maximal-Entropy-Path-Sampler"><a href="#Maximal-Entropy-Path-Sampler" class="headerlink" title="Maximal Entropy Path Sampler"></a>Maximal Entropy Path Sampler</h3><p>为了在考虑效率的情况下获得路径，需要像随机游走的采样策略。然而，传统的随机游走（CRW）的缺点是对不同的节点进行相同的处理，并且忽略了图中节点的中心性。为了解决这些问题，我们提出了在**最大熵随机游走(MERW)**的指导下对路径进行采样。</p>
<p>MERW算法在每一步搜索过程中都是沿着熵率增加的方向搜索路径，并引入了被广泛应用于衡量节点重要性的<strong>特征向量中心度</strong>。</p>
<p>图上随机游走的最大熵率 $\eta$ 可以从转移矩阵 $\mathcal{P}$ 和平稳分布 $\pi$ 计算，其可以描述如下：</p>
<p>$$\eta&#x3D;-\sum_i\pi_i\sum_jp_{ij}\ln p_{ij} \quad(1)$$</p>
<ul>
<li>随机游走的最大熵值有界于 $\text{ln}\lambda$ ，其中 $\lambda$ 是矩阵 $\text{A}$ 的最大特征值</li>
</ul>
<p>为了最大化游走的熵率，MERW将转移概率构造为</p>
<p>$p_{ij} &#x3D; \frac{A_{ij}u_j}{\lambda u_i}$</p>
<ul>
<li>$u&#x3D;(u_{1},u_{2},\cdots,u_{n})$ 为归一化的特征向量</li>
</ul>
<p>注意，转移概率与特征向量中心性成比例，从而保证MERW能够捕获图中节点的结构上下文。它可以被重新表示为最大熵转移（MET）矩阵，其被定义为</p>
<p>$$\mathbf{P}_u&#x3D;\frac{\mathbf{D}_u^{-1}\mathbf{A}\mathbf{D}_u}{\lambda},\quad\quad\quad\quad\quad\quad(2)$$</p>
<ul>
<li>$$\mathbf{D}<em>{u}&#x3D;\mathrm{diag}(u</em>{1},u_{2},\cdots,u_{n})$$</li>
<li>最大熵路径采样器不仅能最大化图的熵率，而且能获得图的复杂结构信息</li>
</ul>
<h3 id="结构感知路径聚合器"><a href="#结构感知路径聚合器" class="headerlink" title="结构感知路径聚合器"></a>结构感知路径聚合器</h3><p>为了聚合路径信息，本文提出了一个结构感知路径聚合器。</p>
<p>具体来说，作者设计了一个结构化的递归单元来编码路径嵌入，它能够将路径中每个节点的<strong>顺序</strong>和<strong>距离</strong>信息结合起来，从而捕获语义信息。</p>
<p>此外，本文的路径偏好模型，区分不同的路径的重要性，实现自适应路径嵌入聚合。</p>
<h4 id="结构感知循环单元"><a href="#结构感知循环单元" class="headerlink" title="结构感知循环单元"></a>结构感知循环单元</h4><p>尽管传统GNN可以聚合高阶邻域的信息，但是它们由于使用同样的方式处理信息，并忽略了图中邻域的全局结构上下文，不可避免地导致了嵌入不可区分。</p>
<p>本文主张每个节点的距离是通过路径捕获结构上下文的关键信息。</p>
<p>本文提出了一个递归单元来编码路径序列，而不是独立地聚合每层节点，从而保留了每个节点的顺序信息。注意，在涉及结构信息之前，本文将所有节点特征 $\mathbf{X}$ 编码为节点嵌入 $\mathbf{I}$ ，其被描述为</p>
<p>$$\mathbf I&#x3D;\sigma\left(\mathbf W_{\mathbf in}\mathbf X+\mathbf b_{\mathbf in}\right) \quad(3)$$</p>
<p>为了捕获基于特征的扩散结构，我们构造结构感知的递归单元 $\Phi $ 为</p>
<p>$$\begin{aligned}<br>&amp;&amp;\mathbf{r}<em>{j}&#x3D;\sigma\left(\mathbf{W}</em>{\mathbf{r}}\cdot h_{j-1}+\mathbf{U}<em>{\mathbf{d}}\cdot\mathbf{I}</em>{j}\right), \<br>&amp;&amp;\mathbf{f}<em>{j}&#x3D;\sigma\left(\mathbf{W}</em>{\mathbf{f}}\cdot h_{j-1}+\mathbf{U}<em>{\mathbf{d}}\cdot\mathbf{I}</em>{j}\right), \<br>&amp;&amp;\mathbf{o}<em>{j}&#x3D;\sigma\left(\mathbf{W</em>{o}}\cdot h_{j-1}+\mathbf{U_{d}}\cdot\mathbf{I}<em>{j}\right), \<br>&amp;&amp;\mathbf{g}</em>{j}&#x3D;\operatorname{tanh}\left(\mathbf{W}<em>{\mathbf{g}}\cdot h</em>{j-1}+\mathbf{U}<em>{\mathbf{d}}\cdot\mathbf{I}</em>{j}\right), \<br>&amp;&amp;\mathbf{c}<em>{j}&#x3D;\mathbf{f}</em>{j}\odot\mathbf{c}<em>{j-1}+\mathbf{r}</em>{j}\odot\mathbf{g}<em>{j}, \<br>&amp;&amp;\mathbf{h}</em>{j}&#x3D;\mathbf{o}<em>{t}\odot\mathrm{tanh}\left(\mathbf{c}</em>{j}\right), \<br>\end{aligned} \quad(4)$$</p>
<ul>
<li><p>$\mathbf{h}_j$ 代表节点 $v$ 在路径中第 $j$ 步的表示</p>
</li>
<li><p>$\odot$ 是哈达玛积</p>
</li>
<li><p>考虑到目标节点的贡献最大，输入顺序与路径的收集顺序相反</p>
</li>
</ul>
<p>考虑到距离信息对结构模式的贡献，采用了<strong>基于节点距离的参数共享机制</strong>的路径嵌入计算。具体地，对于路径中的每个节点，我们使用预先计算的距目标节点的最短距离 $d$ 作为辅助信息，并且具有相同 $d$ 的节点在路径编码期间共享相同的 $\mathbf{U}_d$ ，以便将距离信息并入路径嵌入。</p>
<h4 id="建模路径偏好"><a href="#建模路径偏好" class="headerlink" title="建模路径偏好"></a>建模路径偏好</h4><p>自然的，具有不同同质度水平的邻域具有不同的路径偏好。对于同质性邻域，临近目标节点的路径对于分类贡献更多。然后对于异质性邻域，探索更广泛的图结构是更好的。</p>
<p>因此，我们为每个节点的路径偏好建模，即考虑不同的路径，并学习它们的重要性，以促进自适应嵌入聚合。</p>
<p>具体来说，对于一个特定的路径 $p$ ,我们可以得到它的嵌入 $h_p$ , 然后与目标节点的嵌入 $\mathbf{I}<em>v$ 进行级联，作为通过可训练权重 $a$ 计算偏好系数 $s</em>{v,p}$ 的输入</p>
<p>$$\boldsymbol{s}_{v,p}&#x3D;\text{SOFTMAX}\left(\delta\left(\boldsymbol{a}\left(\mathbf{I}_v|\mathbf{h}_p\right)\right)\right)\quad(5)$$</p>
<p>$\delta$ 为 LeakyReLU </p>
<p>最后，使用偏好系数 $s_{v,p}$ 权衡路径并获取最终的预测 $z_v$</p>
<p>$$\mathbf{z}<em>v&#x3D;\sigma\left(\mathbf{W}</em>{\mathbf{out}}\left(\mathbf{I}<em>v|\sum_\limits{j\in p}s</em>{v,p}\mathbf{h}<em>p\right)+\mathbf{b}</em>{\mathbf{out}}\right)\quad(6)$$</p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/PathNet/" data-id="clnn4xje00004w0u236qh2j43" data-title="PathNet学习" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-HAN学习" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/HAN%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T12:06:58.523Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/HAN%E5%AD%A6%E4%B9%A0/">HAN学习</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="HAN学习"><a href="#HAN学习" class="headerlink" title="HAN学习"></a>HAN学习</h1><p>[TOC]</p>
<h2 id="论文思路"><a href="#论文思路" class="headerlink" title="论文思路"></a>论文思路</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>本文首先提出了一种新的基于分层注意力的异构图神经网络，包括<strong>节点级</strong>和<strong>语义级</strong>的注意力。具体来说，节点级注意力旨在学习<strong>节点</strong>与其基于元路径的<strong>邻居</strong>之间的重要性，而语义级注意力能够学习<strong>不同元路径</strong>的重要性。通过学习节点级和语义级注意力的重要性，可以充分考虑节点和元路径的重要性。然后，所提出的模型可以以<strong>分层的方式</strong>通过基于<strong>源路径的邻居</strong>节点<strong>聚合特征</strong>来生成<strong>节点嵌入</strong>。</p>
<h3 id="HAN模型"><a href="#HAN模型" class="headerlink" title="HAN模型"></a>HAN模型</h3><p>HAN模型使用分层的注意力结构：分为<strong>节点级别</strong>的注意力和<strong>语义级别</strong>的注意力。</p>
<p>节点级别：学习基于元路径的邻居的<strong>权重</strong>并<strong>聚合</strong>它们得到<strong>语义特定的节点嵌入</strong>。</p>
<p>语义级别：判断元路径的<strong>不同</strong>，并针对特定任务得到语义特定节点嵌入的<strong>最优加权组合</strong>。</p>
<p>下图为HAN的整体结构图。</p>
<p><img src="https://img-blog.csdnimg.cn/c249353816544eb3855aeae4543d2e30.png" alt="HAN结构"></p>
<h4 id="节点级别的注意力"><a href="#节点级别的注意力" class="headerlink" title="节点级别的注意力"></a>节点级别的注意力</h4><p>由于每个类型的节点具有不同类型的特征，于是本文设计了类型特定的**转换矩阵$M_{\phi_{i}}$**将不同类型的节点特征映射到同一个特征空间中。映射过程如下公式1：</p>
<p>$$\mathbf{h}<em>i^{\prime}&#x3D;\mathbf{M}</em>{\phi_i}\cdot\mathbf{h}_i $$</p>
<ul>
<li>$\mathbf{h}<em>i$ 和 $\mathbf{h}</em>{i}^{\prime}$ 分别为节点 $i$ 的原始特征和映射后的特征</li>
<li>经过映射之后，节点级别的注意力机制可以处理任意类型的节点。</li>
</ul>
<p>接着，本文使用<strong>自我注意力</strong>学习各种类型节点的权重，给定一个节点对$(i,j)$ ，他们是通过元路径 $\Phi$，$e_{ij}^{\Phi}$代表节点$j$对节点$i$的重要性。如下公式2</p>
<p>$$e_{ij}^{\Phi}&#x3D;att_{node}(\mathbf{h}_i^{\prime},\mathbf{h}_j^{\prime};\Phi)$$</p>
<ul>
<li>这里的$att_{node}$表示执行节点级注意力的深度神经网络</li>
<li>对于指定的$\Phi$,  $att_{node}$是共享的</li>
<li>$e_{ij}^{\Phi}$ 不是对称的</li>
</ul>
<p>然后，通过<strong>隐蔽注意力</strong>将结构信息注入模型（只需要对 $j\in\mathcal{N}<em>{i}^{\Phi}$ 计算 $e</em>{ij}^{\Phi}$）,$$\mathcal{N}_{i}^{\Phi}$$代表节点$i$基于元路径的邻居（包括自己），然后通过softmax函数进行**归一化得到重要性系数$\alpha_{ij}^{\Phi}$**。如下公式3</p>
<p>$$\alpha_{ij}^{\Phi}&#x3D;softmax_{j}(e_{ij}^{\Phi})&#x3D;\frac{\exp(\sigma(\mathbf{a}<em>{\Phi}^{\mathrm{T}}\cdot[\mathbf{h}</em>{i}^{\prime}|\mathbf{h}<em>{j}^{\prime}]))}{\sum</em>{k\in\mathcal{N}<em>{i}^{\Phi}}\exp(\sigma(\mathbf{a}</em>{\Phi}^{\mathrm{T}}\cdot[\mathbf{h}<em>{i}^{\prime}|\mathbf{h}</em>{k}^{\prime}]))}$$</p>
<ul>
<li>$\sigma $ 代表激活函数， $||$ 代表级联</li>
<li>$\mathbf{a}_{\Phi}$ 是元路径$\Phi$对应的节点级别注意力向量</li>
</ul>
<p>然后，节点$i$的<strong>基于元路径的嵌入</strong>可以通过邻居的映射后的特征根据重要性系数进行聚合决定。如下公式4</p>
<p>$$\mathbf{z}<em>i^\Phi&#x3D;\sigma\bigg(\sum</em>{j\in\mathcal{N}<em>i^\Phi}\alpha</em>{ij}^\Phi\cdot\mathbf{h}_j’\bigg)$$</p>
<ul>
<li>$\mathbf{z}_i^\Phi$ 就是学习到的嵌入向量</li>
</ul>
<p>由于异构图具有无标度特性，因此图数据的方差很大。为了解决上述挑战，我们将节点级注意力扩展到多头注意力，使训练过程更加稳定。如下公式5</p>
<p>$$\mathbf{z}<em>{i}^{\Phi}&#x3D;\prod\limits</em>{k&#x3D;1}^{K}\sigma\bigg(\sum\limits_{j\in\mathcal{N}<em>{i}^{\Phi}}\alpha</em>{ij}^{\Phi}\cdot\mathbf{h}_{j}’\bigg)$$</p>
<p>对于元路径集${\Phi_{1},\ldots,\Phi_{P}}$， 在节点的特征经过节点级别注意力之后，我们可以得到$P$组语义特定的节点特征，为$$\left{\mathbf{Z}<em>{\Phi</em>{1}},\ldots,\mathbf{Z}<em>{\Phi</em>{P}}\right}$$</p>
<h4 id="语义级别的注意力"><a href="#语义级别的注意力" class="headerlink" title="语义级别的注意力"></a>语义级别的注意力</h4><p>异构图中的每个节点通常包含<strong>多种类型的语义信息</strong>，语义特定的节点嵌入只能从一个方面反映节点。为了学习一个更全面的节点嵌入，我们需要<strong>融合多种语义</strong>，这些语义可以通过元路径来揭示。为了应对异构图中的元路径选择和语义融合挑战，作者提出了一种新颖的语义级注意力机制，用于自动学习不同元路径的重要性，并将它们融合到特定任务中。</p>
<p>将节点级注意力得到的$P$组语义特定的节点嵌入作为输入，每个元路径的权重可以被描述如下公式6：</p>
<p>$$(\beta_{\Phi_1},\ldots,\beta_{\Phi_P})&#x3D;att_{sem}(\mathrm{Z}<em>{\Phi_1},\ldots,\mathrm{Z}</em>{\Phi_P})$$</p>
<p>为了学习每个元路径的重要性，首先通过非线性变换（例如，单层MLP）变换<strong>语义特定的嵌入向量</strong></p>
<p>然后，我们通过变换嵌入与语义级注意向量$\mathbf q$的<strong>相似性</strong>度量语义特定嵌入的<strong>重要性</strong>。此外，对所有语义特定节点嵌入的重要性进行平均，这可以解释为每个元路径的重要性。如下公式7</p>
<p>$$w_{\Phi_{p}}&#x3D;\frac{1}{|\mathcal{V}|}\sum_{i\in\mathcal{V}}\mathbf{q}^{\mathrm{T}}\cdot\tanh(\mathbf{W}\cdot\mathbf{z}<em>{i}^{\Phi</em>{p}}+\mathbf{b})$$</p>
<ul>
<li>$\mathbf W$是权重矩阵，$\mathbf b$是偏置向量，$\mathbf q$是语义级别的注意力向量</li>
<li>上边的参数都是所有元路径共享的</li>
</ul>
<p>在得到每个元路径的重要性后，通过softmax函数对它们进行归一化。如下公式8</p>
<p>$$\beta_{\Phi_p}&#x3D;\frac{\exp(w_{\Phi_p})}{\sum_{p&#x3D;1}^{P}\exp(w_{\Phi_p})}$$</p>
<ul>
<li>这可以解释为每个元路径在特定任务中的贡献</li>
</ul>
<p>使用学习的权重作为系数，我们可以融合这些语义特定的嵌入以获得最终的嵌入。如下公式9</p>
<p>$$Z&#x3D;\sum_{p&#x3D;1}^{P}\beta_{\Phi_{p}}\cdot Z_{\Phi_{p}}$$</p>
<p>然后，我们可以将最终的嵌入应用于特定的任务，并设计不同的损失函数。</p>
<p>比如，对于半监督的节点分类任务，可以使用<strong>交叉熵损失函数</strong>。如下公式10</p>
<p>$$L&#x3D;-\sum\limits_{l\in\mathcal{Y}_{L}}\mathrm{Y}^{l}\ln(\mathrm{C}\cdot\mathrm{Z}^{l})$$</p>
<p>节点级别的注意力和语义级别的注意力聚合过程图示：</p>
<p><img src="https://img-blog.csdnimg.cn/ca4cf7ec7a9841038f29c6dabff757cb.png" alt="节点级别的注意力和语义级别的注意力聚合过程图示"></p>
<p>HAN的整个过程：</p>
<p><img src="https://img-blog.csdnimg.cn/fd6cbc78ace044beab719a3ac9545a6e.png" alt="HAN算法整个过程"></p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>此模型具体的实现有两种方式：主要区别在于对数据集的处理以及基于元路径邻居的获取。</p>
<ul>
<li><p>直接读取作者处理过的数据集, 可以直接获取基于不同元路径的图</p>
</li>
<li><p>dgl可以通过metapath_reachable_graph方法从一个异质图以及给定的元路径获取源路径的可达图</p>
</li>
</ul>
<p>实现的具体思路相同，下边以第二种方式为例，给出代码（注释中详细解释了实现的思路和过程）:</p>
<blockquote>
<p>下列代码中用到的其他工具类可以在Github仓库中找到</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dgl</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dgl.nn.pytorch <span class="keyword">import</span> GATConv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">注释中的字母代表的含义：</span></span><br><span class="line"><span class="string">N : 节点数量</span></span><br><span class="line"><span class="string">M : 元路径数量</span></span><br><span class="line"><span class="string">D : 嵌入向量维度</span></span><br><span class="line"><span class="string">K : 多头注意力总数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SemanticAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_size, hidden_size=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SemanticAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 语义层次的注意力</span></span><br><span class="line">        <span class="comment"># 对应论文公式（7），最终得到每条元路径的重要性权重</span></span><br><span class="line">        self.projection = nn.Sequential(</span><br><span class="line">            nn.Linear(in_size, hidden_size),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(hidden_size, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="comment"># 输入的z为(N, M, K*D)</span></span><br><span class="line">        <span class="comment"># 经过映射之后的w形状为 (M , 1)</span></span><br><span class="line">        w = self.projection(z).mean(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># beta (M ,1)</span></span><br><span class="line">        beta = torch.softmax(w, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># beta (N, M, 1)</span></span><br><span class="line">        beta = beta.expand((z.shape[<span class="number">0</span>],) + beta.shape)</span><br><span class="line">        <span class="comment"># (N, D*K)</span></span><br><span class="line">        <span class="keyword">return</span> (beta * z).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HANLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    meta_paths : list of metapaths, each as a list of edge types</span></span><br><span class="line"><span class="string">    in_size : input feature dimension</span></span><br><span class="line"><span class="string">    out_size : output feature dimension</span></span><br><span class="line"><span class="string">    layer_num_heads : number of attention heads</span></span><br><span class="line"><span class="string">    dropout : Dropout probability</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 meta_paths,</span></span><br><span class="line"><span class="params">                 in_size,</span></span><br><span class="line"><span class="params">                 out_size,</span></span><br><span class="line"><span class="params">                 layer_num_heads,</span></span><br><span class="line"><span class="params">                 drop_out</span>):</span><br><span class="line">        <span class="built_in">super</span>(HANLayer, self).__init__()</span><br><span class="line">        self.gat_layers = nn.ModuleList()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(meta_paths)):</span><br><span class="line">            <span class="comment"># 使用GAT对应的GATConv层，完成节点层面的注意力</span></span><br><span class="line">            <span class="comment"># 之所以能够之间使用GATConv,是因为在forward中生成了每个元路径对应的可达图</span></span><br><span class="line">            <span class="comment"># 那么在进行节点级注意力的时候，节点的所有邻居都是它基于元路径的邻居</span></span><br><span class="line">            <span class="comment"># 节点级注意力以及聚合的过程就等同于GATConv的过程</span></span><br><span class="line">            self.gat_layers.append(</span><br><span class="line">                GATConv(</span><br><span class="line">                    in_size,</span><br><span class="line">                    out_size,</span><br><span class="line">                    layer_num_heads,</span><br><span class="line">                    drop_out,</span><br><span class="line">                    drop_out,</span><br><span class="line">                    activation=F.elu,</span><br><span class="line">                    allow_zero_in_degree=<span class="literal">True</span></span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 语义级注意力层</span></span><br><span class="line">            self.semantic_attention = SemanticAttention(</span><br><span class="line">                in_size=out_size * layer_num_heads</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            self.meta_paths = <span class="built_in">list</span>(<span class="built_in">tuple</span>(meta_path) <span class="keyword">for</span> meta_path <span class="keyword">in</span> meta_paths)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 缓存图</span></span><br><span class="line">            self._cached_graph = <span class="literal">None</span></span><br><span class="line">            <span class="comment"># 缓存每个元路径对应的可达图</span></span><br><span class="line">            self._cached_coalesced_graph = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g, h</span>):</span><br><span class="line">        semantic_embeddings = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._cached_graph <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> self._cached_graph <span class="keyword">is</span> <span class="keyword">not</span> g:</span><br><span class="line">            self._cached_graph = g</span><br><span class="line">            self._cached_coalesced_graph.clear()</span><br><span class="line">            <span class="comment"># 存储每个元路径对应的元路径可达图</span></span><br><span class="line">            <span class="keyword">for</span> meta_path <span class="keyword">in</span> self.meta_paths:</span><br><span class="line">                self._cached_coalesced_graph[</span><br><span class="line">                    meta_path</span><br><span class="line">                ] = dgl.metapath_reachable_graph(g, meta_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, meta_path <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.meta_paths):</span><br><span class="line">            new_g = self._cached_coalesced_graph[meta_path]</span><br><span class="line">            semantic_embeddings.append(self.gat_layers[i](new_g, h).flatten(<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 经过对每个元路径进行节点级聚合</span></span><br><span class="line">        <span class="comment"># semantic_embeddings 为一个长度为M的列表</span></span><br><span class="line">        <span class="comment"># 其中的元素为每个节点级注意力的输出，形状为(N, D*K)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将该列表在维度1堆叠，得到所有元路径的节点级注意力</span></span><br><span class="line">        <span class="comment"># 形状为(N, M, D * K)</span></span><br><span class="line">        semantic_embeddings = torch.stack(</span><br><span class="line">            semantic_embeddings, dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终经过语义级注意力聚合不同元路径的表示，得到了该层的输出</span></span><br><span class="line">        <span class="comment"># 形状为(N, D * K)</span></span><br><span class="line">        <span class="keyword">return</span> self.semantic_attention(semantic_embeddings)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HAN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    meta_paths : 元路径，使用边类型列表表示</span></span><br><span class="line"><span class="string">    in_size : 输入大小（特征维度）</span></span><br><span class="line"><span class="string">    out_size : 输出大小（节点种类数）</span></span><br><span class="line"><span class="string">    num_heads : 多头注意力头数（列表形式，对应每层的头数）</span></span><br><span class="line"><span class="string">    dropout : dropout概率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, meta_paths, in_size, hidden_size, out_size, num_heads, dropout</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(HAN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        <span class="comment"># 第一个HAN层的输入输出需要单独定义</span></span><br><span class="line">        self.layers.append(</span><br><span class="line">            HANLayer(meta_paths, in_size, hidden_size, num_heads[<span class="number">0</span>], dropout)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 从第二个HAN层开始，每一个层的输入都是hidden_size * 上一个层的头数</span></span><br><span class="line">        <span class="comment"># 输出大小为 hidden_size * 当前层的头数</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(num_heads)):</span><br><span class="line">            self.layers.append(</span><br><span class="line">                HANLayer(</span><br><span class="line">                    meta_paths,</span><br><span class="line">                    hidden_size * num_heads[l - <span class="number">1</span>],</span><br><span class="line">                    hidden_size,</span><br><span class="line">                    num_heads[l],</span><br><span class="line">                    dropout,</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 最终的输出层</span></span><br><span class="line">        self.predict = nn.Linear(hidden_size * num_heads[-<span class="number">1</span>], out_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g, h</span>):</span><br><span class="line">        <span class="keyword">for</span> gnn <span class="keyword">in</span> self.layers:</span><br><span class="line">            h = gnn(g, h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.predict(h)</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/HAN%E5%AD%A6%E4%B9%A0/" data-id="clnn4xjdv0002w0u27d153fi0" data-title="HAN学习" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GraphSAGE学习" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/GraphSAGE%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T12:06:58.521Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/GraphSAGE%E5%AD%A6%E4%B9%A0/">GraphSAGE学习</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="GraphSAGE学习"><a href="#GraphSAGE学习" class="headerlink" title="GraphSAGE学习"></a>GraphSAGE学习</h1><h2 id="1-算法"><a href="#1-算法" class="headerlink" title="1. 算法"></a>1. 算法</h2><p>链接：[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02216">1706.02216] Inductive Representation Learning on Large Graphs</a></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><strong>GraphSAGE</strong>是一个<strong>inductive</strong>的方法，在训练过程中，不会使用测试或者验证集的样本。而<strong>GCN</strong>在训练过程中，会采集测试或者验证集中的样本，因此为<strong>transductive</strong></p>
<h4 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a><strong>GraphSAGE</strong></h4><ol>
<li>对邻居采样</li>
<li>采样后的邻居embedding传到节点上来，并使用一个聚合函数聚合这些邻居信息以更新节点的embedding</li>
<li>根据更新后的embedding预测节点的标签</li>
</ol>
<h4 id="GraphSAGE采样和聚合流程示意图"><a href="#GraphSAGE采样和聚合流程示意图" class="headerlink" title="GraphSAGE采样和聚合流程示意图"></a>GraphSAGE采样和聚合流程示意图</h4><p><img src="https://img-blog.csdnimg.cn/3105dcb5069c43e09f91717d0cad5cf5.png" alt="GraphSAGE采样和聚合流程可视化示意"></p>
<h3 id="嵌入向量生成（前向传播）算法"><a href="#嵌入向量生成（前向传播）算法" class="headerlink" title="嵌入向量生成（前向传播）算法"></a>嵌入向量生成（前向传播）算法</h3><p>本节的内容假设<strong>模型已经完成训练，参数已经固定</strong>。</p>
<p>包括：</p>
<ul>
<li><p><strong>用来聚合节点邻居信息</strong>的$K$个聚合器$\mathrm{AGGREGATE}_k,\forall k\in{1,…,K}$</p>
</li>
<li><p><strong>用来在不同的layer之间传播信息</strong>的$K$个权重矩阵$\mathbf{W}^{k},\forall k\in{1,…,K}$</p>
</li>
</ul>
<p>下图详细描述了前向传播是如何进行的</p>
<ol>
<li>将每个节点的特征向量作为初始的Embedding</li>
<li>对于每个节点，拿到它采样后的邻居的Embedding（$h_u, u \in \mathcal N(v)$）。并聚合邻居的Embedding。<ul>
<li>$$\mathrm{h}_{\mathcal{N}(v)}^k\leftarrow\mathrm{AGGREGATE}_k({\mathbf{h}_u^{k-1},\forall u\in\mathcal{N}(v)})$$</li>
</ul>
</li>
<li>根据聚合后的邻居Embedding $\mathrm{h}_{\mathcal{N}(v)}^k$ 和节点自身的Embedding $h_v^{k-1}$，通过一个非线性变换更新自己的Embedding。<ul>
<li>$$\mathbf{h}<em>{v}^{k}\leftarrow\sigma\left(\mathbf{W}^{k}\cdot\text{coNcAT}(\mathbf{h}</em>{v}^{k-1},\mathbf{h}_{\mathcal{N}(v)}^{k})\right)$$</li>
</ul>
</li>
</ol>
<blockquote>
<p>文中的$K$, 既是聚合器的数量，也是权重矩阵的数量，还是网络的层数。</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/3c150418440f4df9aa25ef7f5d7c686b.png" alt="GraphSAGE算法"></p>
<h4 id="采样算法"><a href="#采样算法" class="headerlink" title="采样算法"></a>采样算法</h4><p>GraphSAGE中的采样是定长的，通过事先定义的邻居个数S， 然后通过有放回的重采样&#x2F;负采样方法。</p>
<p>从而保证：</p>
<ul>
<li><p>可以把节点和他们的邻居拼成tensor送到GPU中训练</p>
</li>
<li><p>计算时每个批次的计算占用空间固定</p>
</li>
<li><p>使时间复杂度变得稳定，原本的时间复杂度可以达到$O(|\mathcal V|)$, 现在可以稳定$O(\prod_{i&#x3D;1}^{K}S_{i}), i \in{1,…,K} $</p>
</li>
</ul>
<h3 id="学习GraphSAGE的参数"><a href="#学习GraphSAGE的参数" class="headerlink" title="学习GraphSAGE的参数"></a>学习GraphSAGE的参数</h3><p>为了在完全无监督的图上进行学习，本文使用了一个基于图的损失函数，来调整$\mathbf{W}^{k}$和聚合器中的参数。</p>
<p>该损失函数鼓励邻近的节点具有相似的表示，并使不同的节点高度区分开。</p>
<p>$$J_{\mathcal{G}}(\mathbf{z}<em>{u})&#x3D;-\log\left(\sigma(\mathbf{z}</em>{u}^{\top}\mathbf{z}<em>{v})\right)-Q\cdot\mathbb{E}</em>{v_{n}\sim P_{n}(v)}\log\left(\sigma(-\mathbf{z}<em>{u}^{\top}\mathbf{z}</em>{v_{n}})\right)$$</p>
<blockquote>
<p>送入该损失函数的嵌入是通过节点的局部邻域中包含的特征生成的，而不是为每个节点生成一个唯一的嵌入。</p>
</blockquote>
<p>如果是<strong>有监督</strong>的情况下，可以使用每个节点的预测lable和真实lable的交叉熵作为损失函数。</p>
<h3 id="聚合器的结构"><a href="#聚合器的结构" class="headerlink" title="聚合器的结构"></a>聚合器的结构</h3><p>与规整的N-D形式不同，节点的邻居没有自然的顺序。因此，聚合函数必须要操作一个<strong>无序的向量集合</strong>。</p>
<p>在理想情况下，聚合器函数将是对称的，同时还是可训练的并且保持高的表示能力。</p>
<p>文章提出了三种候选的聚合器函数：</p>
<ol>
<li><strong>平均聚合器</strong>：简单的取${\mathbf{h}_{u}^{k-1},\forall u\in\mathcal{N}(v)}$中每一个对应位置元素的均值<ul>
<li>$$\mathbf{h}_v^k\leftarrow\sigma(\mathbf{W}\cdot\text{MEAN}({\mathbf{h}_v^{k-1}}\cup{\mathbf{h}_u^{k-1},\forall u\in\mathcal{N}(v)})$$</li>
</ul>
</li>
<li><strong>LSTM聚合器</strong>：与均值聚合器相比，LSTM具有更大的表达能力。但是，它不是对称的。</li>
<li><strong>池化聚合器</strong>：每个邻居的向量独立进入一个全连接神经网络，在经过这个变换之后，应用元素化最大池化操作来聚合跨邻居集合的信息。<ul>
<li>$$\text{AGGREGATE}<em>k^\text{pool}&#x3D;\max(\left{\sigma\left(\mathbf{W}</em>{\mathrm{pool}}\mathbf{h}_{u_i}^k+\mathbf{b}\right),\forall u_i\in\mathcal{N}(v)\right})$$</li>
</ul>
</li>
</ol>
<h2 id="2-总结"><a href="#2-总结" class="headerlink" title="2. 总结"></a>2. 总结</h2><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ol>
<li><strong>使用采样机制，克服了GCN在训练时需要知道全部信息的问题，克服了对显存和内存的限制以及拓展性的问题。</strong></li>
<li>聚合器和权重矩阵的参数对于所有的节点是共享的</li>
<li>模型的参数的数量与图的节点个数无关，这使得GraphSAGE能够处理更大的图</li>
<li>既能处理有监督任务也能处理无监督任务</li>
</ol>
<h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><p>在采样的时候没有考虑不同邻居的重要性</p>
<h2 id="3-SAGEConv的实现"><a href="#3-SAGEConv的实现" class="headerlink" title="3. SAGEConv的实现"></a>3. SAGEConv的实现</h2><p>基于dgl和pytorch的sageconv的实现。包含了四种聚合器，以及对二分图和block同构图的处理。</p>
<blockquote>
<p>此实现参考dgl官方的开源代码，链接在最下方。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> dgl</span><br><span class="line"><span class="keyword">from</span> dgl <span class="keyword">import</span> function <span class="keyword">as</span> fn</span><br><span class="line"><span class="keyword">from</span> dgl.base <span class="keyword">import</span> DGLError</span><br><span class="line"><span class="keyword">from</span> dgl.utils <span class="keyword">import</span> expand_as_pair, check_eq_shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SAGEConv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_feats,</span></span><br><span class="line"><span class="params">                 out_feats,</span></span><br><span class="line"><span class="params">                 aggregator_type,</span></span><br><span class="line"><span class="params">                 feat_drop=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">                 bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 norm=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 activation=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SAGEConv, self).__init__()</span><br><span class="line">        <span class="comment"># 检查聚合器类型是否正确</span></span><br><span class="line">        valid_aggregator_type = &#123;<span class="string">&#x27;mean&#x27;</span>, <span class="string">&#x27;gcn&#x27;</span>, <span class="string">&#x27;pool&#x27;</span>, <span class="string">&#x27;lstm&#x27;</span>&#125;</span><br><span class="line">        <span class="keyword">if</span> aggregator_type <span class="keyword">not</span> <span class="keyword">in</span> valid_aggregator_type:</span><br><span class="line">            <span class="keyword">raise</span> DGLError(</span><br><span class="line">                <span class="string">&quot;Invalid aggregator_type. Must be one of &#123;&#125;. &quot;</span></span><br><span class="line">                <span class="string">&quot;But got &#123;!r&#125; instead.&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    valid_aggregator_type, aggregator_type</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 调用expand_as_pair，如果in_feats是tuple直接返回</span></span><br><span class="line">        <span class="comment"># 如果in_feats是int,则返回两相同此int值，分别代表源、目标节点特征维度</span></span><br><span class="line">        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)</span><br><span class="line">        self._out_feats = out_feats</span><br><span class="line">        self._aggregator_type = aggregator_type</span><br><span class="line">        self.norm = norm</span><br><span class="line">        self.feat_drop = nn.Dropout(feat_drop)</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建聚合器函数</span></span><br><span class="line">        <span class="keyword">if</span> aggregator_type == <span class="string">&quot;pool&quot;</span>:</span><br><span class="line">            self.fc_pool = nn.Linear(self._in_src_feats, self._in_src_feats)</span><br><span class="line">        <span class="keyword">if</span> aggregator_type == <span class="string">&#x27;lstm&#x27;</span>:</span><br><span class="line">            self.lstm = nn.LSTM(self._in_src_feats, self._in_src_feats, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.fc_neigh = nn.Linear(self._in_src_feats, out_feats, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> aggregator_type != <span class="string">&#x27;gcn&#x27;</span>:</span><br><span class="line">            self.fc_self = nn.Linear(self._in_dst_feats, out_feats, bias=bias)</span><br><span class="line">        <span class="keyword">elif</span> bias:</span><br><span class="line">            self.bias = nn.parameter.Parameter(torch.zeros(self._out_feats))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_buffer(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Reinitialize learnable parameters.&quot;&quot;&quot;</span></span><br><span class="line">        gain = nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> self._aggregator_type == <span class="string">&#x27;pool&#x27;</span>:</span><br><span class="line">            nn.init.xavier_uniform_(self.fc_pool.weight, gain=gain)</span><br><span class="line">        <span class="keyword">if</span> self._aggregator_type == <span class="string">&#x27;lstm&#x27;</span>:</span><br><span class="line">            self.lstm.reset_parameters()</span><br><span class="line">        <span class="keyword">if</span> self._aggregator_type != <span class="string">&#x27;gcn&#x27;</span>:</span><br><span class="line">            nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)</span><br><span class="line">        nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_lstm_reducer</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        实现一个LSTM聚合器</span></span><br><span class="line"><span class="string">        :param nodes: 邻居节点</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># m形状为（B, L, D）</span></span><br><span class="line">        <span class="comment"># B : batch_size</span></span><br><span class="line">        <span class="comment"># L : num of neighbors</span></span><br><span class="line">        <span class="comment"># D : dims of features</span></span><br><span class="line">        m = nodes.mailbox[<span class="string">&quot;m&quot;</span>]</span><br><span class="line">        batch_size = m.shape[<span class="number">0</span>]</span><br><span class="line">        h = (</span><br><span class="line">            m.new_zeros((<span class="number">1</span>, batch_size, self._in_src_feats)),</span><br><span class="line">            m.new_zeros((<span class="number">1</span>, batch_size, self._in_src_feats))</span><br><span class="line">        )</span><br><span class="line">        _, (rst, _) = self.lstm(m, h)</span><br><span class="line">        <span class="comment"># rst形状为（B, D）</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;neigh&quot;</span>: rst.squeeze(<span class="number">0</span>)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph, feat, edge_weight=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compute GraphSAGE Layer</span></span><br><span class="line"><span class="string">        :param graph: 图</span></span><br><span class="line"><span class="string">        :param feat: 特征 （N, D_in）或 二分图（N_in, D_in_src）(N_out, D_out_src)</span></span><br><span class="line"><span class="string">        :param edge_weight: 边权</span></span><br><span class="line"><span class="string">        :return: 本层输出的特征（N_dst, D_out）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> graph.local_scope():</span><br><span class="line">            <span class="comment"># 判断输入的feat是哪一种</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(feat, <span class="built_in">tuple</span>):  <span class="comment"># 单二分图</span></span><br><span class="line">                feat_src = self.feat_drop(feat[<span class="number">0</span>])</span><br><span class="line">                feat_dst = self.feat_drop(feat[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                feat_src = feat_dst = self.feat_drop(feat)  <span class="comment"># 同构图</span></span><br><span class="line">                <span class="comment"># 同构图的block情况</span></span><br><span class="line">                <span class="keyword">if</span> graph.is_block:</span><br><span class="line">                    feat_dst = feat_src[:graph.number_of_dst_nodes()]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 定义一个消息传播函数</span></span><br><span class="line">            msg_fn = fn.copy_u(<span class="string">&quot;h&quot;</span>, <span class="string">&#x27;m&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果有边权，则调用内置u_mul_e，把起点的h特征乘以边权重，再将结果赋给边的m特征</span></span><br><span class="line">            <span class="keyword">if</span> edge_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">assert</span> edge_weight.shape[<span class="number">0</span>] == graph.num_edges()</span><br><span class="line">                graph.edata[<span class="string">&quot;_edge_weight&quot;</span>] = edge_weight</span><br><span class="line">                msg_fn = fn.u_mul_e(<span class="string">&quot;h&quot;</span>, <span class="string">&quot;_edge_weight&quot;</span>, <span class="string">&quot;m&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 记录目标节点的原始特征</span></span><br><span class="line">            h_self = feat_dst</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 处理无边图的情况</span></span><br><span class="line">            <span class="keyword">if</span> graph.num_edges() == <span class="number">0</span>:</span><br><span class="line">                graph.dstdata[<span class="string">&#x27;neigh&#x27;</span>] = torch.zeros(</span><br><span class="line">                    feat_dst.shape[<span class="number">0</span>], self._in_src_feats</span><br><span class="line">                ).to(feat_dst)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 确定在消息传播之前是否应用线性转换</span></span><br><span class="line">            <span class="comment"># 如果输入特征的维度大于输出特征的维度，需要先通过一个线性层转换维度</span></span><br><span class="line">            lin_before_mp = self._in_src_feats &gt; self._out_feats</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 消息传播</span></span><br><span class="line">            <span class="keyword">if</span> self._aggregator_type == <span class="string">&#x27;mean&#x27;</span>:</span><br><span class="line">                <span class="comment"># 将特征置于节点中的‘h’中</span></span><br><span class="line">                <span class="comment"># 如果需要降维, 使用fc_neigh</span></span><br><span class="line">                graph.srcdata[<span class="string">&quot;h&quot;</span>] = (self.fc_neigh(feat_src) <span class="keyword">if</span> lin_before_mp <span class="keyword">else</span> feat_src)</span><br><span class="line">                <span class="comment"># 通过消息传播更新模型</span></span><br><span class="line">                <span class="comment"># 将h复制给m, 对邻居的m求均值，然后赋值给neigh</span></span><br><span class="line">                graph.update_all(msg_fn, fn.mean(<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;neigh&#x27;</span>))</span><br><span class="line">                h_neigh = graph.dstdata[<span class="string">&quot;neigh&quot;</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> lin_before_mp:</span><br><span class="line">                    h_neigh = self.fc_neigh(h_neigh)</span><br><span class="line">            <span class="keyword">elif</span> self._aggregator_type == <span class="string">&#x27;gcn&#x27;</span>:</span><br><span class="line">                <span class="comment"># 检查源节点和目标节点的形状是否一直</span></span><br><span class="line">                check_eq_shape(feat)</span><br><span class="line">                graph.srcdata[<span class="string">&quot;h&quot;</span>] = (</span><br><span class="line">                    self.fc_neigh(feat_src) <span class="keyword">if</span> lin_before_mp <span class="keyword">else</span> feat_src</span><br><span class="line">                )</span><br><span class="line">                <span class="comment"># 是否为二分图</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(feat, <span class="built_in">tuple</span>):</span><br><span class="line">                    graph.dstdata[<span class="string">&#x27;h&#x27;</span>] = (</span><br><span class="line">                        self.fc_neigh(feat_dst) <span class="keyword">if</span> lin_before_mp <span class="keyword">else</span> feat_dst</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> graph.is_block:  <span class="comment"># 同构图block的情况</span></span><br><span class="line">                        graph.dst_data[<span class="string">&quot;h&quot;</span>] = graph.srcdata[<span class="string">&quot;h&quot;</span>][:graph.num_dst_nodes()]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        graph.dstdata[<span class="string">&#x27;h&#x27;</span>] = graph.srcdata[<span class="string">&#x27;h&#x27;</span>]</span><br><span class="line">                <span class="comment"># 将h复制到m, 然后把邻居节点的m聚合起来赋值为neigh</span></span><br><span class="line">                graph.update_all(msg_fn, fn.<span class="built_in">sum</span>(<span class="string">&quot;m&quot;</span>, <span class="string">&quot;neigh&quot;</span>))</span><br><span class="line">                <span class="comment"># 除以入度</span></span><br><span class="line">                degs = graph.in_degrees().to(feat_dst)</span><br><span class="line">                h_neigh = (graph.dstdata[<span class="string">&#x27;neigh&#x27;</span>] + graph.dstdata[<span class="string">&#x27;h&#x27;</span>]) / (degs.unsqueeze(-<span class="number">1</span>) + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> lin_before_mp:</span><br><span class="line">                    h_neigh = self.fc_neigh(h_neigh)</span><br><span class="line">            <span class="keyword">elif</span> self._aggregator_type == <span class="string">&#x27;pool&#x27;</span>:</span><br><span class="line">                <span class="comment"># 将feat_src经过一个池化和激活函数放进h</span></span><br><span class="line">                graph.srcdata[<span class="string">&#x27;h&#x27;</span>] = F.relu(self.fc_pool(feat_src))</span><br><span class="line">                <span class="comment"># h复制到m, 然后使用最大化聚合m和neigh</span></span><br><span class="line">                graph.update_all(msg_fn, fn.<span class="built_in">max</span>(<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;neigh&#x27;</span>))</span><br><span class="line">                <span class="comment"># 对聚合结果进行一个线性转化</span></span><br><span class="line">                h_neigh = self.fc_neigh(graph.dstdata[<span class="string">&#x27;neigh&#x27;</span>])</span><br><span class="line">            <span class="keyword">elif</span> self._aggregator_type == <span class="string">&quot;lstm&quot;</span>:</span><br><span class="line">                graph.srcdata[<span class="string">&quot;h&quot;</span>] = feat_src</span><br><span class="line">                <span class="comment"># 通过自己设置的lstm-reduce聚合</span></span><br><span class="line">                graph.update_all(msg_fn, self._lstm_reducer)</span><br><span class="line">                h_neigh = self.fc_neigh(graph.dstdata[<span class="string">&quot;neigh&quot;</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> KeyError(</span><br><span class="line">                    <span class="string">&quot;Aggregator type &#123;&#125; not recognized.&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                        self._aggre_type</span><br><span class="line">                    )</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># GraphSAGE GCN 不需要fc_self</span></span><br><span class="line">            <span class="keyword">if</span> self._aggregator_type == <span class="string">&#x27;gcn&#x27;</span>:</span><br><span class="line">                rst = h_neigh</span><br><span class="line">                <span class="comment"># 手动为GCN添加偏置</span></span><br><span class="line">                <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    rst = rst + self.bias</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                rst = self.fc_self(h_self) + h_neigh</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 激活函数</span></span><br><span class="line">            <span class="keyword">if</span> self.activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                rst = self.activation(rst)</span><br><span class="line">            <span class="comment"># 归一化</span></span><br><span class="line">            <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                rst = self.norm(rst)</span><br><span class="line">            <span class="keyword">return</span> rst</span><br></pre></td></tr></table></figure>

<h2 id="4-模型训练代码"><a href="#4-模型训练代码" class="headerlink" title="4. 模型训练代码"></a>4. 模型训练代码</h2><p>训练模型代码：</p>
<p>可以选择在<strong>cora,citeseer,pubmed</strong>上训练，模型结构为包含两个gcn聚合的sageconv层。</p>
<p>结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* cora: ~0.8330</span><br><span class="line">* citeseer: ~0.7110</span><br><span class="line">* pubmed: ~0.7830</span><br></pre></td></tr></table></figure>

<blockquote>
<p>此代码为全图训练</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> dgl</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> sageconv <span class="keyword">import</span> SAGEConv</span><br><span class="line"><span class="keyword">from</span> dgl <span class="keyword">import</span> AddSelfLoop</span><br><span class="line"><span class="keyword">from</span> dgl.data <span class="keyword">import</span> CiteseerGraphDataset, CoraGraphDataset, PubmedGraphDataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SAGE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_size, hid_size, out_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        <span class="comment"># 一个两层的SAGE</span></span><br><span class="line">        self.layers.append(SAGEConv(in_size, hid_size, <span class="string">&#x27;gcn&#x27;</span>))</span><br><span class="line">        self.layers.append(SAGEConv(hid_size, out_size, <span class="string">&#x27;gcn&#x27;</span>))</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph, x</span>):</span><br><span class="line">        h = self.dropout(x)</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layers):</span><br><span class="line">            h = layer(graph, h)</span><br><span class="line">            <span class="keyword">if</span> i != <span class="built_in">len</span>(self.layers) - <span class="number">1</span>:</span><br><span class="line">                h = F.relu(h)</span><br><span class="line">                h = self.dropout(h)</span><br><span class="line">        <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">g, features, labels, mask, model</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        logits = model(g, features)</span><br><span class="line">        logits = logits[mask]</span><br><span class="line">        labels = labels[mask]</span><br><span class="line">        _, indices = torch.<span class="built_in">max</span>(logits, dim=<span class="number">1</span>)</span><br><span class="line">        correct = torch.<span class="built_in">sum</span>(indices == labels)</span><br><span class="line">        <span class="keyword">return</span> correct.item() * <span class="number">1.0</span> / <span class="built_in">len</span>(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">g, features, labels, masks, model</span>):</span><br><span class="line">    <span class="comment"># 划分训练集/验证集，损失函数和优化器</span></span><br><span class="line">    train_mask, val_mask = masks</span><br><span class="line">    loss_fcn = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(),</span><br><span class="line">                                 lr=<span class="number">1e-2</span>,</span><br><span class="line">                                 weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">    <span class="comment"># train loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        logits = model(g, features)</span><br><span class="line">        loss = loss_fcn(logits[train_mask], labels[train_mask])</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        acc = evaluate(g, features, labels, val_mask, model)</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Epoch &#123;:05d&#125; | Loss &#123;:.4f&#125; | Accuracy &#123;:.4f&#125; &quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, loss.item(), acc</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&quot;GraphSAGE&quot;</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--dataset&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="string">&#x27;cora&#x27;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Dataset name (&#x27;cora&#x27;, &#x27;citeseer&#x27;, &#x27;pubmed&#x27;)&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--dt&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="string">&quot;float&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;data type(float, bfloat16)&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training with GraphSAGE module based on dgl&quot;</span>)</span><br><span class="line">    <span class="comment"># load and preprocess dataset</span></span><br><span class="line">    transform = (</span><br><span class="line">        AddSelfLoop()</span><br><span class="line">    )  <span class="comment"># by default, it will first remove self-loops to prevent duplication</span></span><br><span class="line">    <span class="keyword">if</span> args.dataset == <span class="string">&quot;cora&quot;</span>:</span><br><span class="line">        data = CoraGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">elif</span> args.dataset == <span class="string">&quot;citeseer&quot;</span>:</span><br><span class="line">        data = CiteseerGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">elif</span> args.dataset == <span class="string">&quot;pubmed&quot;</span>:</span><br><span class="line">        data = PubmedGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Unknown dataset: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(args.dataset))</span><br><span class="line">    g = data[<span class="number">0</span>]</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    g = g.<span class="built_in">int</span>().to(device)</span><br><span class="line">    features = g.ndata[<span class="string">&quot;feat&quot;</span>]</span><br><span class="line">    labels = g.ndata[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">    masks = g.ndata[<span class="string">&quot;train_mask&quot;</span>], g.ndata[<span class="string">&quot;val_mask&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create GraphSAGE model</span></span><br><span class="line">    in_size = features.shape[<span class="number">1</span>]</span><br><span class="line">    out_size = data.num_classes</span><br><span class="line">    model = SAGE(in_size, <span class="number">16</span>, out_size).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert model and graph to bfloat16 if needed</span></span><br><span class="line">    <span class="keyword">if</span> args.dt == <span class="string">&quot;bfloat16&quot;</span>:</span><br><span class="line">        g = dgl.to_bfloat16(g)</span><br><span class="line">        features = features.to(dtype=torch.bfloat16)</span><br><span class="line">        model = model.to(dtype=torch.bfloat16)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model training</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training...&quot;</span>)</span><br><span class="line">    train(g, features, labels, masks, model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test the model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Testing...&quot;</span>)</span><br><span class="line">    acc = evaluate(g, features, labels, g.ndata[<span class="string">&quot;test_mask&quot;</span>], model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test accuracy &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(acc))</span><br></pre></td></tr></table></figure>

<h2 id="4-参考链接"><a href="#4-参考链接" class="headerlink" title="4. 参考链接"></a>4. 参考链接</h2><p><a target="_blank" rel="noopener" href="https://archwalker.github.io/blog/2019/06/01/GNN-Triplets-GraphSAGE.html">GNN 教程：GraphSAGE - ArchWalker</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/136521625">图神经网络从入门到入门 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/sageconv.html#SAGEConv">dgl.nn.pytorch.conv.sageconv — DGL 1.1.1 documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/dmlc/dgl/blob/1.1.x/examples/pytorch/graphsage/README.md">dgl的官方示例</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/GraphSAGE%E5%AD%A6%E4%B9%A0/" data-id="clnn4xjdz0003w0u28xlpcqaf" data-title="GraphSAGE学习" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GCN学习" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/GCN%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T12:06:58.519Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/GCN%E5%AD%A6%E4%B9%A0/">GCN学习</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="GCN代码简析（dgl实现）"><a href="#GCN代码简析（dgl实现）" class="headerlink" title="GCN代码简析（dgl实现）"></a>GCN代码简析（dgl实现）</h1><h2 id="1-GCN"><a href="#1-GCN" class="headerlink" title="1. GCN"></a>1. GCN</h2><p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.02907">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a></p>
<p>GCN的逐层传播公式为：$$H^{(l+1)}&#x3D;\sigma\left(\tilde{D}^{-\frac12}\tilde{A}\tilde{D}^{-\frac12}H^{(l)}W^{(l)}\right)$$</p>
<p>其中：</p>
<ul>
<li><p>$\tilde A &#x3D; A + I_N$ 表示无向图G加上了自己的邻接矩阵</p>
</li>
<li><p>$\tilde D_{ii} &#x3D; \sum_j \tilde A_{ij}$ 是节点的度</p>
</li>
<li><p>$W^{(l)}$ 是l层的可学习参数</p>
</li>
<li><p>$H^{(l)} \in \mathbb R^{N \times D}$ 是l层激活后的节点Embedding，$H_{0} &#x3D; X$</p>
</li>
</ul>
<p>推导过程参考：</p>
<p><a target="_blank" rel="noopener" href="https://archwalker.github.io/blog/2019/06/01/GNN-Triplets-GCN.html">GNN 教程：GCN - ArchWalker</a></p>
<p>其中切比雪夫多项式近似的部分可以参考：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/106687580">Chebyshev多项式作为GCN卷积核</a></p>
<p>如果想更多了解一些GCN相关的原理可以参考：</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/54504471/answer/332657604">如何理解 Graph Convolutional Network（GCN）？</a></p>
<h2 id="2-GCN代码实现与GraphConv源码解读"><a href="#2-GCN代码实现与GraphConv源码解读" class="headerlink" title="2. GCN代码实现与GraphConv源码解读"></a>2. GCN代码实现与GraphConv源码解读</h2><h3 id="GCN代码实现"><a href="#GCN代码实现" class="headerlink" title="GCN代码实现"></a>GCN代码实现</h3><p>GCN代码为官方给出的示例，地址为：<a target="_blank" rel="noopener" href="https://github.com/dmlc/dgl/tree/1.1.x/examples/pytorch/gcn">dgl官方GCN实例</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> dgl</span><br><span class="line"><span class="keyword">import</span> dgl.nn <span class="keyword">as</span> dglnn</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> dgl <span class="keyword">import</span> AddSelfLoop</span><br><span class="line"><span class="keyword">from</span> dgl.data <span class="keyword">import</span> CiteseerGraphDataset, CoraGraphDataset, PubmedGraphDataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GCN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_size, hid_size, out_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(GCN).__init__()</span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        <span class="comment"># two-layer GCN</span></span><br><span class="line">        <span class="comment"># 定义了一个两层的GCN</span></span><br><span class="line">        self.layers.append(</span><br><span class="line">            dglnn.GraphConv(in_size, hid_size, activation=F.relu)</span><br><span class="line">        )</span><br><span class="line">        self.layers.append(dglnn.GraphConv(hid_size, out_size))</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g, features</span>):</span><br><span class="line">        h = features</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layers):</span><br><span class="line">            <span class="comment"># 在第一层传播之前不进行dropout</span></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span>:</span><br><span class="line">                h = self.dropout(h)</span><br><span class="line">            h = layer(g, h)</span><br><span class="line">        <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">g, features, labels, mask, model</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        logits = model(g, features)</span><br><span class="line">        logits = logits[mask]</span><br><span class="line">        labels = labels[mask]</span><br><span class="line">        _, indices = torch.<span class="built_in">max</span>(logits, dim=<span class="number">1</span>)</span><br><span class="line">        correct = torch.<span class="built_in">sum</span>(indices == labels)</span><br><span class="line">        <span class="keyword">return</span> correct.item() * <span class="number">1.0</span> / <span class="built_in">len</span>(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">g, features, labels, masks, model</span>):</span><br><span class="line">    <span class="comment"># define train/val samples, loss function and optimizer</span></span><br><span class="line">    train_mask = masks[<span class="number">0</span>]</span><br><span class="line">    val_mask = masks[<span class="number">1</span>]</span><br><span class="line">    loss_fcn = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-2</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># training loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        logits = model(g, features)</span><br><span class="line">        loss = loss_fcn(logits[train_mask], labels[train_mask])</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        acc = evaluate(g, features, labels, val_mask, model)</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Epoch &#123;:05d&#125; | Loss &#123;:.4f&#125; | Accuracy &#123;:.4f&#125; &quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, loss.item(), acc</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--dataset&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="string">&quot;cora&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Dataset name (&#x27;cora&#x27;, &#x27;citeseer&#x27;, &#x27;pubmed&#x27;).&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--dt&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="string">&quot;float&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;data type(float, bfloat16)&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Training with DGL built-in GraphConv module.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load and preprocess dataset</span></span><br><span class="line">    <span class="comment"># 在加载数组集前对图添加自环</span></span><br><span class="line">    transform = (</span><br><span class="line">        AddSelfLoop()</span><br><span class="line">    )  <span class="comment"># by default, it will first remove self-loops to prevent duplication</span></span><br><span class="line">    <span class="keyword">if</span> args.dataset == <span class="string">&quot;cora&quot;</span>:</span><br><span class="line">        data = CoraGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">elif</span> args.dataset == <span class="string">&quot;citeseer&quot;</span>:</span><br><span class="line">        data = CiteseerGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">elif</span> args.dataset == <span class="string">&quot;pubmed&quot;</span>:</span><br><span class="line">        data = PubmedGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Unknown dataset: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(args.dataset))</span><br><span class="line">    g = data[<span class="number">0</span>]</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    g = g.<span class="built_in">int</span>().to(device)</span><br><span class="line">    features = g.ndata[<span class="string">&quot;feat&quot;</span>]</span><br><span class="line">    labels = g.ndata[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">    masks = g.ndata[<span class="string">&quot;train_mask&quot;</span>], g.ndata[<span class="string">&quot;val_mask&quot;</span>], g.ndata[<span class="string">&quot;test_mask&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create GCN model</span></span><br><span class="line">    in_size = features.shape[<span class="number">1</span>]</span><br><span class="line">    out_size = data.num_classes</span><br><span class="line">    model = GCN(in_size, <span class="number">16</span>, out_size).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert model and graph to bfloat16 if needed</span></span><br><span class="line">    <span class="keyword">if</span> args.dt == <span class="string">&quot;bfloat16&quot;</span>:</span><br><span class="line">        g = dgl.to_bfloat16(g)</span><br><span class="line">        features = features.to(dtype=torch.bfloat16)</span><br><span class="line">        model = model.to(dtype=torch.bfloat16)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model training</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training...&quot;</span>)</span><br><span class="line">    train(g, features, labels, masks, model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test the model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Testing...&quot;</span>)</span><br><span class="line">    acc = evaluate(g, features, labels, masks[<span class="number">2</span>], model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test accuracy &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(acc))</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3 id="GraphConv源码解读"><a href="#GraphConv源码解读" class="headerlink" title="GraphConv源码解读"></a>GraphConv源码解读</h3><p>下方为dgl中<code>GraphConv</code>层的源码，此代码为dgl(1.1.1版本的实现)，官方文档地址为：<a target="_blank" rel="noopener" href="https://docs.dgl.ai/en/1.1.x/generated/dgl.nn.pytorch.conv.GraphConv.html">GraphConv</a>，本文对源码添加了注释。</p>
<blockquote>
<p>注释为个人理解，如发现错误请指正</p>
</blockquote>
<p>文档中提及的无权图的逐层传播公式为</p>
<p>$h_i^{(l+1)} &#x3D; \sigma(b^{(l)} + \sum_{j\in\mathcal{N}(i)}\frac{1}{c_{ji}}h_j^{(l)}W^{(l)})$</p>
<p>此公式为在节点层面的表示</p>
<p>其中$c_{ji}$为节点度的平方根因子，$c_{ji} &#x3D; \sqrt{|\mathcal{N}(j)|}\sqrt{|\mathcal{N}(i)|}$。</p>
<p>此处的逐层传播公式实际上与上边的公式含义一致</p>
<p>$$H^{(l+1)}&#x3D;\sigma\left(\tilde{D}^{-\frac12}\tilde{A}\tilde{D}^{-\frac12}H^{(l)}W^{(l)}\right)$$该式中的$\tilde{D}^{-\frac12}\tilde{A}\tilde{D}^{-\frac12}$实际上为对称归一化的拉普拉斯矩阵$L_{sym}$，而$L_{sym}$可以表示为</p>
<p>$$L_{i,j}^{\text{sym}}:&#x3D;\left{\begin{matrix}1&amp;\text{if} \ i&#x3D;j \ \text {and}\deg(v_i)\neq0\\frac{1}{\sqrt{\deg(v_i)\deg(v_j)}}&amp;\text{if} \ i\neq j \ \text{and} \ v_i \ \text{is adjacent to} \ v_j\0&amp;\text{otherwise}\end{matrix}\right.$$</p>
<p>其中，$v_i$和$v_j$相连时，所乘的因子就相当于$c_{ij}$，由于在dgl中使用消息传播机制，因此不需要乘整个矩阵，只需要对对应的元素做出归一化即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphConv</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Graph convolutional layer from `Semi-Supervised Classification with Graph Convolutional</span></span><br><span class="line"><span class="string">    Networks &lt;https://arxiv.org/abs/1609.02907&gt;`__</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Mathematically it is defined as follows:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">      h_i^&#123;(l+1)&#125; = \sigma(b^&#123;(l)&#125; + \sum_&#123;j\in\mathcal&#123;N&#125;(i)&#125;\frac&#123;1&#125;&#123;c_&#123;ji&#125;&#125;h_j^&#123;(l)&#125;W^&#123;(l)&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`\mathcal&#123;N&#125;(i)` is the set of neighbors of node :math:`i`,</span></span><br><span class="line"><span class="string">    :math:`c_&#123;ji&#125;` is the product of the square root of node degrees</span></span><br><span class="line"><span class="string">    (i.e.,  :math:`c_&#123;ji&#125; = \sqrt&#123;|\mathcal&#123;N&#125;(j)|&#125;\sqrt&#123;|\mathcal&#123;N&#125;(i)|&#125;`),</span></span><br><span class="line"><span class="string">    and :math:`\sigma` is an activation function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If a weight tensor on each edge is provided, the weighted graph convolution is defined as:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">      h_i^&#123;(l+1)&#125; = \sigma(b^&#123;(l)&#125; + \sum_&#123;j\in\mathcal&#123;N&#125;(i)&#125;\frac&#123;e_&#123;ji&#125;&#125;&#123;c_&#123;ji&#125;&#125;h_j^&#123;(l)&#125;W^&#123;(l)&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`e_&#123;ji&#125;` is the scalar weight on the edge from node :math:`j` to node :math:`i`.</span></span><br><span class="line"><span class="string">    This is NOT equivalent to the weighted graph convolutional network formulation in the paper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    To customize the normalization term :math:`c_&#123;ji&#125;`, one can first set ``norm=&#x27;none&#x27;`` for</span></span><br><span class="line"><span class="string">    the model, and send the pre-normalized :math:`e_&#123;ji&#125;` to the forward computation. We provide</span></span><br><span class="line"><span class="string">    :class:`~dgl.nn.pytorch.EdgeWeightNorm` to normalize scalar edge weight following the GCN paper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    in_feats : int</span></span><br><span class="line"><span class="string">        Input feature size; i.e, the number of dimensions of :math:`h_j^&#123;(l)&#125;`.</span></span><br><span class="line"><span class="string">    out_feats : int</span></span><br><span class="line"><span class="string">        Output feature size; i.e., the number of dimensions of :math:`h_i^&#123;(l+1)&#125;`.</span></span><br><span class="line"><span class="string">    norm : str, optional</span></span><br><span class="line"><span class="string">        How to apply the normalizer.  Can be one of the following values:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * ``right``, to divide the aggregated messages by each node&#x27;s in-degrees,</span></span><br><span class="line"><span class="string">          which is equivalent to averaging the received messages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * ``none``, where no normalization is applied.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * ``both`` (default), where the messages are scaled with :math:`1/c_&#123;ji&#125;` above, equivalent</span></span><br><span class="line"><span class="string">          to symmetric normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * ``left``, to divide the messages sent out from each node by its out-degrees,</span></span><br><span class="line"><span class="string">          equivalent to random walk normalization.</span></span><br><span class="line"><span class="string">    weight : bool, optional</span></span><br><span class="line"><span class="string">        If True, apply a linear layer. Otherwise, aggregating the messages</span></span><br><span class="line"><span class="string">        without a weight matrix.</span></span><br><span class="line"><span class="string">    bias : bool, optional</span></span><br><span class="line"><span class="string">        If True, adds a learnable bias to the output. Default: ``True``.</span></span><br><span class="line"><span class="string">    activation : callable activation function/layer or None, optional</span></span><br><span class="line"><span class="string">        If not None, applies an activation function to the updated node features.</span></span><br><span class="line"><span class="string">        Default: ``None``.</span></span><br><span class="line"><span class="string">    allow_zero_in_degree : bool, optional</span></span><br><span class="line"><span class="string">        If there are 0-in-degree nodes in the graph, output for those nodes will be invalid</span></span><br><span class="line"><span class="string">        since no message will be passed to those nodes. This is harmful for some applications</span></span><br><span class="line"><span class="string">        causing silent performance regression. This module will raise a DGLError if it detects</span></span><br><span class="line"><span class="string">        0-in-degree nodes in input graph. By setting ``True``, it will suppress the check</span></span><br><span class="line"><span class="string">        and let the users handle it by themselves. Default: ``False``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    weight : torch.Tensor</span></span><br><span class="line"><span class="string">        The learnable weight tensor.</span></span><br><span class="line"><span class="string">    bias : torch.Tensor</span></span><br><span class="line"><span class="string">        The learnable bias tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note</span></span><br><span class="line"><span class="string">    ----</span></span><br><span class="line"><span class="string">    Zero in-degree nodes will lead to invalid output value. This is because no message</span></span><br><span class="line"><span class="string">    will be passed to those nodes, the aggregation function will be appied on empty input.</span></span><br><span class="line"><span class="string">    A common practice to avoid this is to add a self-loop for each node in the graph if</span></span><br><span class="line"><span class="string">    it is homogeneous, which can be achieved by:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; g = ... # a DGLGraph</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; g = dgl.add_self_loop(g)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Calling ``add_self_loop`` will not work for some graphs, for example, heterogeneous graph</span></span><br><span class="line"><span class="string">    since the edge type can not be decided for self_loop edges. Set ``allow_zero_in_degree``</span></span><br><span class="line"><span class="string">    to ``True`` for those cases to unblock the code and handle zero-in-degree nodes manually.</span></span><br><span class="line"><span class="string">    A common practise to handle this is to filter out the nodes with zero-in-degree when use</span></span><br><span class="line"><span class="string">    after conv.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; import dgl</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; import numpy as np</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; import torch as th</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; from dgl.nn import GraphConv</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; # Case 1: Homogeneous graph</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; g = dgl.add_self_loop(g)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; feat = th.ones(6, 10)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; conv = GraphConv(10, 2, norm=&#x27;both&#x27;, weight=True, bias=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; res = conv(g, feat)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; print(res)</span></span><br><span class="line"><span class="string">    tensor([[ 1.3326, -0.2797],</span></span><br><span class="line"><span class="string">            [ 1.4673, -0.3080],</span></span><br><span class="line"><span class="string">            [ 1.3326, -0.2797],</span></span><br><span class="line"><span class="string">            [ 1.6871, -0.3541],</span></span><br><span class="line"><span class="string">            [ 1.7711, -0.3717],</span></span><br><span class="line"><span class="string">            [ 1.0375, -0.2178]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; # allow_zero_in_degree example</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; conv = GraphConv(10, 2, norm=&#x27;both&#x27;, weight=True, bias=True, allow_zero_in_degree=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; res = conv(g, feat)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; print(res)</span></span><br><span class="line"><span class="string">    tensor([[-0.2473, -0.4631],</span></span><br><span class="line"><span class="string">            [-0.3497, -0.6549],</span></span><br><span class="line"><span class="string">            [-0.3497, -0.6549],</span></span><br><span class="line"><span class="string">            [-0.4221, -0.7905],</span></span><br><span class="line"><span class="string">            [-0.3497, -0.6549],</span></span><br><span class="line"><span class="string">            [ 0.0000,  0.0000]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; # Case 2: Unidirectional bipartite graph</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; u = [0, 1, 0, 0, 1]</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; v = [0, 1, 2, 3, 2]</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; g = dgl.heterograph(&#123;(&#x27;_U&#x27;, &#x27;_E&#x27;, &#x27;_V&#x27;) : (u, v)&#125;)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; u_fea = th.rand(2, 5)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; v_fea = th.rand(4, 5)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; conv = GraphConv(5, 2, norm=&#x27;both&#x27;, weight=True, bias=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; res = conv(g, (u_fea, v_fea))</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; res</span></span><br><span class="line"><span class="string">    tensor([[-0.2994,  0.6106],</span></span><br><span class="line"><span class="string">            [-0.4482,  0.5540],</span></span><br><span class="line"><span class="string">            [-0.5287,  0.8235],</span></span><br><span class="line"><span class="string">            [-0.2994,  0.6106]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_feats,</span></span><br><span class="line"><span class="params">        out_feats,</span></span><br><span class="line"><span class="params">        norm=<span class="string">&quot;both&quot;</span>,</span></span><br><span class="line"><span class="params">        weight=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        activation=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        allow_zero_in_degree=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(GraphConv, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> norm <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&quot;none&quot;</span>, <span class="string">&quot;both&quot;</span>, <span class="string">&quot;right&quot;</span>, <span class="string">&quot;left&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> DGLError(</span><br><span class="line">                <span class="string">&#x27;Invalid norm value. Must be either &quot;none&quot;, &quot;both&quot;, &quot;right&quot; or &quot;left&quot;.&#x27;</span></span><br><span class="line">                <span class="string">&#x27; But got &quot;&#123;&#125;&quot;.&#x27;</span>.<span class="built_in">format</span>(norm)</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 输入维度</span></span><br><span class="line">        self._in_feats = in_feats</span><br><span class="line">        <span class="comment"># 输出维度</span></span><br><span class="line">        self._out_feats = out_feats</span><br><span class="line">        <span class="comment"># 归一化方式</span></span><br><span class="line">        <span class="comment"># right: 将聚合的消息除以结点的入度，相当于平均接受的消息</span></span><br><span class="line">        <span class="comment"># none: 不使用</span></span><br><span class="line">        <span class="comment"># both: 默认方式，消息将通过1/c_&#123;ij&#125;被缩放，相当于对称归一化</span></span><br><span class="line">        <span class="comment"># left: 将聚合的消息除以结点的出度，相当于随机游走归一化</span></span><br><span class="line">        self._norm = norm</span><br><span class="line">        <span class="comment"># 是否允许度为0的结点</span></span><br><span class="line">        self._allow_zero_in_degree = allow_zero_in_degree</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 是否使用权重</span></span><br><span class="line">        <span class="keyword">if</span> weight:</span><br><span class="line">            self.weight = nn.Parameter(th.Tensor(in_feats, out_feats))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&quot;weight&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 是否使用偏置</span></span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(th.Tensor(out_feats))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&quot;bias&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化可学习参数</span></span><br><span class="line">        self.reset_parameters()</span><br><span class="line">        <span class="comment"># 激活函数</span></span><br><span class="line">        self._activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Description</span></span><br><span class="line"><span class="string">        -----------</span></span><br><span class="line"><span class="string">        Reinitialize learnable parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Note</span></span><br><span class="line"><span class="string">        ----</span></span><br><span class="line"><span class="string">        The model parameters are initialized as in the</span></span><br><span class="line"><span class="string">        `original implementation &lt;https://github.com/tkipf/gcn/blob/master/gcn/layers.py&gt;`__</span></span><br><span class="line"><span class="string">        where the weight :math:`W^&#123;(l)&#125;` is initialized using Glorot uniform initialization</span></span><br><span class="line"><span class="string">        and the bias is initialized to be zero.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            init.xavier_uniform_(self.weight)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            init.zeros_(self.bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_allow_zero_in_degree</span>(<span class="params">self, set_value</span>):</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Description</span></span><br><span class="line"><span class="string">        -----------</span></span><br><span class="line"><span class="string">        Set allow_zero_in_degree flag.</span></span><br><span class="line"><span class="string">        设置是否允许度为0的标志</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        set_value : bool</span></span><br><span class="line"><span class="string">            The value to be set to the flag.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self._allow_zero_in_degree = set_value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph, feat, weight=<span class="literal">None</span>, edge_weight=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Description</span></span><br><span class="line"><span class="string">        -----------</span></span><br><span class="line"><span class="string">        Compute graph convolution.</span></span><br><span class="line"><span class="string">        图卷积计算</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        graph : DGLGraph</span></span><br><span class="line"><span class="string">            The graph.</span></span><br><span class="line"><span class="string">        feat :</span></span><br><span class="line"><span class="string">            结点的特征，对同质图为（N, D）</span></span><br><span class="line"><span class="string">            N为结点个数， D为输入结点特征的维度</span></span><br><span class="line"><span class="string">            torch.Tensor or pair of torch.Tensor</span></span><br><span class="line"><span class="string">            If a torch.Tensor is given, it represents the input feature of shape</span></span><br><span class="line"><span class="string">            :math:`(N, D_&#123;in&#125;)`</span></span><br><span class="line"><span class="string">            where :math:`D_&#123;in&#125;` is size of input feature, :math:`N` is the number of nodes.</span></span><br><span class="line"><span class="string">            If a pair of torch.Tensor is given, which is the case for bipartite graph, the pair</span></span><br><span class="line"><span class="string">            must contain two tensors of shape :math:`(N_&#123;in&#125;, D_&#123;in_&#123;src&#125;&#125;)` and</span></span><br><span class="line"><span class="string">            :math:`(N_&#123;out&#125;, D_&#123;in_&#123;dst&#125;&#125;)`.</span></span><br><span class="line"><span class="string">        weight :</span></span><br><span class="line"><span class="string">            可选参数，为卷积层提供一个权重，若卷积层已经存在一个权重，那么会抛出一个错误</span></span><br><span class="line"><span class="string">            torch.Tensor, optional</span></span><br><span class="line"><span class="string">            Optional external weight tensor.</span></span><br><span class="line"><span class="string">        edge_weight :</span></span><br><span class="line"><span class="string">            可选参数，边权重</span></span><br><span class="line"><span class="string">            torch.Tensor, optional</span></span><br><span class="line"><span class="string">            Optional tensor on the edge. If given, the convolution will weight</span></span><br><span class="line"><span class="string">            with regard to the message.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        返回输出特征</span></span><br><span class="line"><span class="string">        torch.Tensor</span></span><br><span class="line"><span class="string">            The output feature</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Raises</span></span><br><span class="line"><span class="string">        ------</span></span><br><span class="line"><span class="string">        DGLError</span></span><br><span class="line"><span class="string">            Case 1: 图中有度为0的结点，可以通过设置allow_zero=True来允许有度为0的结点。</span></span><br><span class="line"><span class="string">            If there are 0-in-degree nodes in the input graph, it will raise DGLError</span></span><br><span class="line"><span class="string">            since no message will be passed to those nodes. This will cause invalid output.</span></span><br><span class="line"><span class="string">            The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            Case 2: 从外部提供权重的同时模块定义了自己的权重</span></span><br><span class="line"><span class="string">            External weight is provided while at the same time the module</span></span><br><span class="line"><span class="string">            has defined its own weight parameter.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Note</span></span><br><span class="line"><span class="string">        ----</span></span><br><span class="line"><span class="string">        * 输入形状: (N, *, \text&#123;in_feats&#125;)</span></span><br><span class="line"><span class="string">          Input shape: :math:`(N, *, \text&#123;in_feats&#125;)` where * means any number of additional</span></span><br><span class="line"><span class="string">          dimensions, :math:`N` is the number of nodes.</span></span><br><span class="line"><span class="string">        * 输出形状: (N, *, \text&#123;out_feats&#125;)</span></span><br><span class="line"><span class="string">          Output shape: :math:`(N, *, \text&#123;out_feats&#125;)` where all but the last dimension are</span></span><br><span class="line"><span class="string">          the same shape as the input.</span></span><br><span class="line"><span class="string">        * 权重形状: (\text&#123;in_feats&#125;, \text&#123;out_feats&#125;)</span></span><br><span class="line"><span class="string">          Weight shape: :math:`(\text&#123;in_feats&#125;, \text&#123;out_feats&#125;)`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> graph.local_scope():  <span class="comment"># 使用局部范围，不影响图中节点的值</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self._allow_zero_in_degree:  <span class="comment"># 如果不允许输入度为0的结点</span></span><br><span class="line">                <span class="keyword">if</span> (graph.in_degrees() == <span class="number">0</span>).<span class="built_in">any</span>():</span><br><span class="line">                    <span class="keyword">raise</span> DGLError(</span><br><span class="line">                        <span class="string">&quot;There are 0-in-degree nodes in the graph, &quot;</span></span><br><span class="line">                        <span class="string">&quot;output for those nodes will be invalid. &quot;</span></span><br><span class="line">                        <span class="string">&quot;This is harmful for some applications, &quot;</span></span><br><span class="line">                        <span class="string">&quot;causing silent performance regression. &quot;</span></span><br><span class="line">                        <span class="string">&quot;Adding self-loop on the input graph by &quot;</span></span><br><span class="line">                        <span class="string">&quot;calling `g = dgl.add_self_loop(g)` will resolve &quot;</span></span><br><span class="line">                        <span class="string">&quot;the issue. Setting ``allow_zero_in_degree`` &quot;</span></span><br><span class="line">                        <span class="string">&quot;to be `True` when constructing this module will &quot;</span></span><br><span class="line">                        <span class="string">&quot;suppress the check and let the code run.&quot;</span></span><br><span class="line">                    )</span><br><span class="line">            <span class="comment"># 定义聚合函数</span></span><br><span class="line">            aggregate_fn = fn.copy_u(<span class="string">&quot;h&quot;</span>, <span class="string">&quot;m&quot;</span>)</span><br><span class="line">            <span class="comment"># 如果边有权重</span></span><br><span class="line">            <span class="keyword">if</span> edge_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">assert</span> edge_weight.shape[<span class="number">0</span>] == graph.num_edges()</span><br><span class="line">                graph.edata[<span class="string">&quot;_edge_weight&quot;</span>] = edge_weight</span><br><span class="line">                <span class="comment"># 更换聚合函数，使特征乘以边权后再聚合</span></span><br><span class="line">                aggregate_fn = fn.u_mul_e(<span class="string">&quot;h&quot;</span>, <span class="string">&quot;_edge_weight&quot;</span>, <span class="string">&quot;m&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.</span></span><br><span class="line">            <span class="comment"># 将特征分解为源节点特征和目标节点特征</span></span><br><span class="line">            feat_src, feat_dst = expand_as_pair(feat, graph)</span><br><span class="line">            <span class="comment"># 如果使用的归一化方式为‘left’或’both‘</span></span><br><span class="line">            <span class="keyword">if</span> self._norm <span class="keyword">in</span> [<span class="string">&quot;left&quot;</span>, <span class="string">&quot;both&quot;</span>]:</span><br><span class="line">                <span class="comment"># 获取所有结点的出度</span></span><br><span class="line">                <span class="comment"># todo: to(feat_src)的作用和含义？</span></span><br><span class="line">                degs = graph.out_degrees().to(feat_src).clamp(<span class="built_in">min</span>=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># 按归一化类型设置参数</span></span><br><span class="line">                <span class="keyword">if</span> self._norm == <span class="string">&quot;both&quot;</span>:</span><br><span class="line">                    <span class="comment"># D^(-1/2)</span></span><br><span class="line">                    norm = th.<span class="built_in">pow</span>(degs, -<span class="number">0.5</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    norm = <span class="number">1.0</span> / degs</span><br><span class="line">                <span class="comment"># 修改norm的形状方便与feat_src相乘</span></span><br><span class="line">                shp = norm.shape + (<span class="number">1</span>,) * (feat_src.dim() - <span class="number">1</span>)</span><br><span class="line">                norm = th.reshape(norm, shp)</span><br><span class="line">                <span class="comment"># 相当于 D^(-1/2)* feat_src</span></span><br><span class="line">                feat_src = feat_src * norm</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 上文提到的raise DGLError的case2</span></span><br><span class="line">            <span class="keyword">if</span> weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> self.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">raise</span> DGLError(</span><br><span class="line">                        <span class="string">&quot;External weight is provided while at the same time the&quot;</span></span><br><span class="line">                        <span class="string">&quot; module has defined its own weight parameter. Please&quot;</span></span><br><span class="line">                        <span class="string">&quot; create the module with flag weight=False.&quot;</span></span><br><span class="line">                    )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                weight = self.weight</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果输入维度大于输出维度, 就先乘W, 减小特征向量的大小方便聚合</span></span><br><span class="line">            <span class="comment"># 否则就先聚合再乘以权重</span></span><br><span class="line">            <span class="keyword">if</span> self._in_feats &gt; self._out_feats:</span><br><span class="line">                <span class="comment"># mult W first to reduce the feature size for aggregation.</span></span><br><span class="line">                <span class="keyword">if</span> weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    feat_src = th.matmul(feat_src, weight)</span><br><span class="line">                <span class="comment"># 将源节点特征放入特征域&#x27;h&#x27;</span></span><br><span class="line">                graph.srcdata[<span class="string">&quot;h&quot;</span>] = feat_src</span><br><span class="line">                <span class="comment"># 通过上边定义的聚合函数, 首先将&#x27;h&#x27;的特征复制到&#x27;m&#x27;, 然后将&#x27;m&#x27;的特征聚合存放在&#x27;h&#x27;</span></span><br><span class="line">                graph.update_all(aggregate_fn, fn.<span class="built_in">sum</span>(msg=<span class="string">&quot;m&quot;</span>, out=<span class="string">&quot;h&quot;</span>))</span><br><span class="line">                rst = graph.dstdata[<span class="string">&quot;h&quot;</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># aggregate first then mult W</span></span><br><span class="line">                <span class="comment"># 与上边相同, 执行顺序不同, 优化计算</span></span><br><span class="line">                graph.srcdata[<span class="string">&quot;h&quot;</span>] = feat_src</span><br><span class="line">                graph.update_all(aggregate_fn, fn.<span class="built_in">sum</span>(msg=<span class="string">&quot;m&quot;</span>, out=<span class="string">&quot;h&quot;</span>))</span><br><span class="line">                rst = graph.dstdata[<span class="string">&quot;h&quot;</span>]</span><br><span class="line">                <span class="keyword">if</span> weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    rst = th.matmul(rst, weight)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># right或者both</span></span><br><span class="line">            <span class="keyword">if</span> self._norm <span class="keyword">in</span> [<span class="string">&quot;right&quot;</span>, <span class="string">&quot;both&quot;</span>]:</span><br><span class="line">                degs = graph.in_degrees().to(feat_dst).clamp(<span class="built_in">min</span>=<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> self._norm == <span class="string">&quot;both&quot;</span>:</span><br><span class="line">                    norm = th.<span class="built_in">pow</span>(degs, -<span class="number">0.5</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    norm = <span class="number">1.0</span> / degs</span><br><span class="line">                shp = norm.shape + (<span class="number">1</span>,) * (feat_dst.dim() - <span class="number">1</span>)</span><br><span class="line">                norm = th.reshape(norm, shp)</span><br><span class="line">                rst = rst * norm <span class="comment"># 同上，rst * D^(-1/2)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果有偏置，加上偏置值</span></span><br><span class="line">            <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                rst = rst + self.bias</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 通过设置的激活函数进行激活</span></span><br><span class="line">            <span class="keyword">if</span> self._activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                rst = self._activation(rst)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> rst</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Set the extra representation of the module,</span></span><br><span class="line"><span class="string">        which will come into effect when printing the model.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        summary = <span class="string">&quot;in=&#123;_in_feats&#125;, out=&#123;_out_feats&#125;&quot;</span></span><br><span class="line">        summary += <span class="string">&quot;, normalization=&#123;_norm&#125;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;_activation&quot;</span> <span class="keyword">in</span> self.__dict__:</span><br><span class="line">            summary += <span class="string">&quot;, activation=&#123;_activation&#125;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> summary.<span class="built_in">format</span>(**self.__dict__)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/GCN%E5%AD%A6%E4%B9%A0/" data-id="clnn4xjds0001w0u200wybi8w" data-title="GCN学习" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GAT学习" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/GAT%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T12:06:58.518Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/GAT%E5%AD%A6%E4%B9%A0/">GAT学习</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="GAT学习"><a href="#GAT学习" class="headerlink" title="GAT学习"></a>GAT学习</h1><h2 id="论文内容"><a href="#论文内容" class="headerlink" title="论文内容"></a>论文内容</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a></p>
<h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p>本文提出了一种用在图结构数据上的新的神经网络结构。</p>
<p>此方法利用掩蔽的自注意层来解决基于图卷积或其近似的现有方法的缺点。</p>
<p>通过堆叠层，节点能够关注到他们的邻域的特征，允许（隐式地）为邻域中的不同节点指定不同的权重，而不需要任何昂贵的矩阵运算（例如求逆），也不需要事先知道图结构。</p>
<p><strong>解决了邻域节点重要性不同的问题</strong></p>
<h3 id="图注意力层-Graph-Attention-Layer"><a href="#图注意力层-Graph-Attention-Layer" class="headerlink" title="图注意力层 Graph Attention Layer"></a>图注意力层 Graph Attention Layer</h3><p>GAL的输入是一组节点特征，$\mathbf{h}&#x3D;{\vec{h}<em>{1},\vec{h}</em>{2},\ldots,\vec{h}<em>{N}},\vec{h}</em>{i}\in\mathbb{R}^{F}$，其中$N$是节点的个数，$F$是每个节点的特征的个数。</p>
<p>输出：$$\mathbf{h}^{\prime}&#x3D;{\vec{h}<em>{1}^{\prime},\vec{h}</em>{2}^{\prime},\ldots,\vec{h}<em>{N}^{\prime}},\vec{\vec{h}}</em>{i}^{\prime}\in\mathbb{R}^{F^{\prime}}$$</p>
<h4 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h4><p>为了确保学习到足够的表达，至少需要一个线性转换将输入特征转换为更高级别的特征。最终，作为初始步骤，对于每个节点使用一个共享的线性表示，权重矩阵为$\mathbf{W}\in\mathbb{R}^{F^{\prime}\times F}$</p>
<p>然后对节点使用<strong>self-attention</strong>（一种共享注意力机制）$a:\mathbb{R}^{F^{‘}}\times\mathbb{R}^{F^{‘}}\to\mathbb{R}$计算<strong>注意力系数</strong>。</p>
<p>$e_{ij}&#x3D;a(\mathbf{W}\vec{h}_i,\mathbf{W}\vec{h}_j)$这个公式代表节点$j$的特征对节点$i$的重要程度。本文只计算每个节点的一阶邻居节点对该节点的重要性。</p>
<p>为了使这个重要性系数易于比较，本文使用softmax函数对他们进行归一化$\alpha_{ij}&#x3D;\mathrm{softmax}<em>j(e</em>{ij})&#x3D;\frac{\exp(e_{ij})}{\sum_{k\in\mathcal{N}<em>i}\exp(e</em>{ik})}$</p>
<p>注意力机制$a$是一个单层前向神经网络，权重向量为$\vec{\mathbf{a}}\in\mathbb{R}^{2F^{\prime}}$，并使用<strong>LeakyReLU</strong>非线性激活函数。</p>
<p>最终，结合上述的公式和内容，计算系数的公式如下</p>
<p>$$\alpha_{ij}&#x3D;\frac{\exp\left(\text{LeakyReLU}\left(\vec{\mathbf{a}}^T[\textbf{W}\vec{h}_i|\textbf{W}\vec{h}<em>j]\right)\right)}{\sum</em>{k\in\mathcal{N}_i}\exp\left(\text{LeakyReLU}\left(\vec{\mathbf{a}}^T[\textbf{W}\vec{h}_i|\textbf{W}\vec{h}_k]\right)\right)}$$</p>
<p>其中节点i的向量为$\vec{h}_i$形状为$(F, 1)$, $\textbf{W}\vec{h}_i$ 形状为$(F^{\prime}, 1)$</p>
<p>将两个节点转换后的向量连在一起后形状为$(2F^{\prime}, 1 )$</p>
<p>与转置后的注意力向量$(1,2F^{\prime})$相乘得到重要性系数，最终通过激活函数和softmax归一化后得到最终的系数的值。</p>
<p>最终输出的向量为，每个邻居节点的<strong>注意力系数</strong>乘以对应节点经过线性变化得到的<strong>特征向量</strong>，全部加起来之后经过一个<strong>激活函数</strong>。公式如下：</p>
<p>$$\vec{h}<em>i’&#x3D;\sigma\left(\sum</em>{j\in\mathcal{N}<em>i}\alpha</em>{ij}\mathbf{W}\vec{h}_j\right)$$</p>
<p><img src="https://img-blog.csdnimg.cn/5f987e59303b4b68b86035df1e525ccf.png" alt="注意力机制的图示"></p>
<h4 id="拓展注意力机制"><a href="#拓展注意力机制" class="headerlink" title="拓展注意力机制"></a>拓展注意力机制</h4><p>为了稳定自我注意力机制的学习过程，作者发现通过使用<strong>多头注意力</strong>对于该机制是有益的。</p>
<p>具体来说，<strong>K个独立注意力机制</strong>执行上述的输出过程，然后它们的特征被<strong>级联</strong>，如下所示：</p>
<p>$$\vec{h}<em>i’&#x3D;\prod\limits</em>{k&#x3D;1}^K\sigma\left(\sum\limits_{j\in\mathcal{N}<em>i}\alpha</em>{ij}^k\mathbf{W}^k\vec{h}_j\right)$$</p>
<p>最终返回每个节点的$h^{\prime}$包含$KF^{\prime}$个特征。</p>
<p><strong>注意</strong>，如果是最后一层，那么不应该级联，而应该<strong>求均值</strong>。</p>
<p><img src="https://img-blog.csdnimg.cn/6ec991e69df84d09b712031026d80276.png" alt="多头注意力机制图解"></p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ul>
<li>GAT的效率很高，自注意力层的操作可以在所有的边上一起进行，输出特征的计算可以在所有节点上同时进行。不需要特征分解等消耗较大的矩阵运算。时间复杂度可以表示为$O(|V|{F}F^{\prime}{+}|E|F^{\prime})$，与GCN相当。</li>
<li>与GCN相比，GAT为同一个邻域中的节点分配了不同的<strong>重要性权重</strong>，从而使模型能力产生飞跃。</li>
<li>注意力机制以共享的方式应用于图中的所有边，并且因此它不依赖于对全局图结构或其所有节点的预先访问（许多现有技术的限制）。使GAT是<strong>inductive</strong>的。</li>
<li>相比于其他inductive的方法,GraphSAGE,取一个<strong>固定大小的邻域</strong>，这使GraphSAGE不能获取整个邻域。GAT不受此限制，它与整个邻域一起工作。</li>
<li>使用节点的特征进行相似性计算，而不是节点的结构属性。</li>
</ul>
<p>未来工作的方向：</p>
<p>本文生成一个利用稀疏矩阵运算的GAT层版本，将节点和边的数量的存储复杂性降低到线性，并能够在更大的图形数据集上执行GAT模型。</p>
<p>然而，我们使用的张量操作框架仅支持秩为2的张量的稀疏矩阵乘法，这限制了该层当前实现的批处理能力（特别是对于具有多个图的数据集）。</p>
<p>妥善解决这一制约因素是今后工作的重要方向。取决于适当位置的图结构的规则性，在这些稀疏场景中，与CPU相比，GPU可能无法提供主要的性能优势。</p>
<p>还应该注意的是，模型的“感受野”的大小是由网络的深度上限（类似于GCN和类似模型）。诸如跳过连接的技术（Resnet）可以容易地应用于适当地延伸深度。最后，跨所有图边缘的并行化（特别是以分布式方式）可能涉及大量冗余计算，因为邻域通常在感兴趣的图中高度重叠。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="GATConv实现"><a href="#GATConv实现" class="headerlink" title="GATConv实现"></a>GATConv实现</h3><p>GATConv实现（对于代码的理解都在注释中）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> dgl.function <span class="keyword">as</span> fn</span><br><span class="line"><span class="keyword">from</span> dgl.base <span class="keyword">import</span> DGLError</span><br><span class="line"><span class="keyword">from</span> dgl.utils <span class="keyword">import</span> expand_as_pair</span><br><span class="line"><span class="keyword">from</span> dgl.nn.functional <span class="keyword">import</span> edge_softmax</span><br><span class="line"><span class="keyword">from</span> dgl.nn.pytorch.utils <span class="keyword">import</span> Identity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GATConv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            in_feats,</span></span><br><span class="line"><span class="params">            out_feats,</span></span><br><span class="line"><span class="params">            num_heads,</span></span><br><span class="line"><span class="params">            feat_drop=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">            attn_drop=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">            negative_slope=<span class="number">0.2</span>,</span></span><br><span class="line"><span class="params">            residual=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            activation=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            allow_zero_in_degree=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            bias=<span class="literal">True</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(GATConv, self).__init__()</span><br><span class="line">        self._num_heads = num_heads</span><br><span class="line">        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)</span><br><span class="line">        self._out_feats = out_feats</span><br><span class="line">        self._allow_zero_in_degree = allow_zero_in_degree</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(in_feats, <span class="built_in">tuple</span>):</span><br><span class="line">            <span class="comment"># 二分图</span></span><br><span class="line">            <span class="comment"># 分别为两个特征创建一个线性转换层</span></span><br><span class="line">            self.fc_src = nn.Linear(</span><br><span class="line">                self._in_src_feats, out_feats * num_heads, bias=<span class="literal">False</span></span><br><span class="line">            )</span><br><span class="line">            self.fc_dst = nn.Linear(</span><br><span class="line">                self._in_dst_feats, out_feats * num_heads, bias=<span class="literal">False</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 正常图直接创建一个线性转换层，相当于论文中的W</span></span><br><span class="line">            self.fc = nn.Linear(</span><br><span class="line">                self._in_src_feats, out_feats * num_heads, bias=<span class="literal">False</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建注意力向量</span></span><br><span class="line">        <span class="comment"># dgl在实现时将源节点和目标节点的注意力分数，分开计算，因此有两个注意力向量</span></span><br><span class="line">        <span class="comment"># 详细的解释在forward()函数中</span></span><br><span class="line">        self.attn_l = nn.Parameter(</span><br><span class="line">            torch.FloatTensor(size=(<span class="number">1</span>, num_heads, out_feats))</span><br><span class="line">        )</span><br><span class="line">        self.attn_r = nn.Parameter(</span><br><span class="line">            torch.FloatTensor(size=(<span class="number">1</span>, num_heads, out_feats))</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout层</span></span><br><span class="line">        self.feat_drop = nn.Dropout(feat_drop)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        <span class="comment"># 激活函数</span></span><br><span class="line">        self.leaky_relu = nn.LeakyReLU(negative_slope)</span><br><span class="line"></span><br><span class="line">        self.has_linear_res = <span class="literal">False</span></span><br><span class="line">        self.has_explicit_bias = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化多头自注意力层的参数，包括残差连接和偏置</span></span><br><span class="line">        <span class="keyword">if</span> residual:</span><br><span class="line">            <span class="keyword">if</span> self._in_dst_feats != out_feats * num_heads:</span><br><span class="line">                self.res_fc = nn.Linear(</span><br><span class="line">                    self._in_dst_feats, num_heads * out_feats, bias=bias</span><br><span class="line">                )</span><br><span class="line">                self.has_linear_res = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.res_fc = Identity()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_buffer(<span class="string">&quot;res_fc&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">and</span> <span class="keyword">not</span> self.has_linear_res:</span><br><span class="line">            self.bias = nn.Parameter(torch.zeros(num_heads * out_feats, ))</span><br><span class="line">            self.has_explicit_bias = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_buffer(<span class="string">&quot;bias&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        self.reset_parameters()</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        重新初始化可学习参数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        gain = nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;fc&quot;</span>):</span><br><span class="line">            nn.init.xavier_normal_(self.fc.weight, gain=gain)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)</span><br><span class="line">            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)</span><br><span class="line"></span><br><span class="line">        nn.init.xavier_normal_(self.attn_l, gain=gain)</span><br><span class="line">        nn.init.xavier_normal_(self.attn_r, gain=gain)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.has_explicit_bias:</span><br><span class="line">            nn.init.constant_(self.bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self.res_fc, nn.Linear):</span><br><span class="line">            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)</span><br><span class="line">            <span class="keyword">if</span> self.res_fc.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.init.constant_(self.res_fc.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_allow_zero_in_degree</span>(<span class="params">self, set_value</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        设置是否为0的标志</span></span><br><span class="line"><span class="string">        :param set_value:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self._allow_zero_in_degree = set_value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph, feat, edge_weight=<span class="literal">None</span>, get_attention=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        :param graph: 图</span></span><br><span class="line"><span class="string">        :param feat: 节点特征</span></span><br><span class="line"><span class="string">        :param edge_weight: 边权</span></span><br><span class="line"><span class="string">        :param get_attention: 是否返回注意力</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> graph.local_scope():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self._allow_zero_in_degree:</span><br><span class="line">                <span class="keyword">if</span> (graph.in_degrees() == <span class="number">0</span>).<span class="built_in">any</span>():</span><br><span class="line">                    <span class="keyword">raise</span> DGLError(</span><br><span class="line">                        <span class="string">&quot;There are 0-in-degree nodes in the graph, &quot;</span></span><br><span class="line">                        <span class="string">&quot;output for those nodes will be invalid. &quot;</span></span><br><span class="line">                        <span class="string">&quot;This is harmful for some applications, &quot;</span></span><br><span class="line">                        <span class="string">&quot;causing silent performance regression. &quot;</span></span><br><span class="line">                        <span class="string">&quot;Adding self-loop on the input graph by &quot;</span></span><br><span class="line">                        <span class="string">&quot;calling `g = dgl.add_self_loop(g)` will resolve &quot;</span></span><br><span class="line">                        <span class="string">&quot;the issue. Setting ``allow_zero_in_degree`` &quot;</span></span><br><span class="line">                        <span class="string">&quot;to be `True` when constructing this module will &quot;</span></span><br><span class="line">                        <span class="string">&quot;suppress the check and let the code run.&quot;</span></span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 该部分处理输入的特征，将特征经过线性转换后，存放在feat_src, feat_dst</span></span><br><span class="line">            <span class="comment"># 相当于 W h_i</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(feat, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="comment"># 二分图的情况</span></span><br><span class="line">                src_prefix_shape = feat[<span class="number">0</span>].shape[:-<span class="number">1</span>]</span><br><span class="line">                dst_prefix_shape = feat[<span class="number">1</span>].shape[:-<span class="number">1</span>]</span><br><span class="line">                h_src = self.feat_drop(feat[<span class="number">0</span>])</span><br><span class="line">                h_dst = self.feat_drop(feat[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;fc_src&#x27;</span>):</span><br><span class="line">                    feat_src = self.fc(h_src).view(</span><br><span class="line">                        *src_prefix_shape, self._num_heads, self._out_feats</span><br><span class="line">                    )</span><br><span class="line">                    feat_dst = self.fc(h_dst).view(</span><br><span class="line">                        *dst_prefix_shape, self._num_heads, self._out_feats</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    feat_src = self.fc_src(h_src).view(</span><br><span class="line">                        *src_prefix_shape, self._num_heads, self._out_feats</span><br><span class="line">                    )</span><br><span class="line">                    feat_dst = self.fc_dst(h_dst).view(</span><br><span class="line">                        *dst_prefix_shape, self._num_heads, self._out_feats</span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 同构图</span></span><br><span class="line">                <span class="comment"># 获取源节点和目标节点的个数</span></span><br><span class="line">                src_prefix_shape = dst_prefix_shape = feat.shape[:-<span class="number">1</span>]</span><br><span class="line">                <span class="comment"># 源节点和目标节点的特征（经过dropout）</span></span><br><span class="line">                h_src = h_dst = self.feat_drop(feat)</span><br><span class="line">                <span class="comment"># 经过fc层后形状为(num_nodes, out_feats * num_heads)</span></span><br><span class="line">                <span class="comment"># 将形状变为(num_nodes, num_heads, out_feats)</span></span><br><span class="line">                feat_src = feat_dst = self.fc(h_src).view(</span><br><span class="line">                    *src_prefix_shape, self._num_heads, self._out_feats</span><br><span class="line">                )</span><br><span class="line">                <span class="comment"># 对block的处理</span></span><br><span class="line">                <span class="keyword">if</span> graph.is_block:</span><br><span class="line">                    feat_dst = feat_src[: graph.number_of_dst_nodes()]</span><br><span class="line">                    h_dst = h_dst[: graph.number_of_dst_nodes()]</span><br><span class="line">                    dst_prefix_shape = (</span><br><span class="line">                                           graph.number_of_dst_nodes(),</span><br><span class="line">                                       ) + dst_prefix_shape[<span class="number">1</span>:]</span><br><span class="line">            <span class="comment"># <span class="doctag">NOTE:</span> GAT paper uses &quot;first concatenation then linear projection&quot;</span></span><br><span class="line">            <span class="comment"># to compute attention scores, while ours is &quot;first projection then</span></span><br><span class="line">            <span class="comment"># addition&quot;, the two approaches are mathematically equivalent:</span></span><br><span class="line">            <span class="comment"># We decompose the weight vector a mentioned in the paper into</span></span><br><span class="line">            <span class="comment"># [a_l || a_r], then</span></span><br><span class="line">            <span class="comment"># a^T [Wh_i || Wh_j] = a_l Wh_i + a_r Wh_j</span></span><br><span class="line">            <span class="comment"># Our implementation is much efficient because we do not need to</span></span><br><span class="line">            <span class="comment"># save [Wh_i || Wh_j] on edges, which is not memory-efficient. Plus,</span></span><br><span class="line">            <span class="comment"># addition could be optimized with DGL&#x27;s built-in function u_add_v,</span></span><br><span class="line">            <span class="comment"># which further speeds up computation and saves memory footprint.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算注意力</span></span><br><span class="line">            <span class="comment"># 此部分对应论文中的公式（1）-（3）</span></span><br><span class="line">            <span class="comment"># (num_nodes, num_heads, out_feat) * (1, num_heads, out_feat)</span></span><br><span class="line">            <span class="comment"># 广播到形状一致后逐元素乘积, 将最后一个维度加起来，然后再添加一个维度</span></span><br><span class="line">            <span class="comment"># 最终el为(num_nodes, num_heads, 1)</span></span><br><span class="line">            <span class="comment"># 最后一个维度就是每个节点，每个注意力头对应的注意力分数</span></span><br><span class="line">            <span class="comment"># 这样完成了所有节点和所有注意力头同时计算</span></span><br><span class="line">            el = (feat_src * self.attn_l).<span class="built_in">sum</span>(dim=-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 目标节点计算注意力</span></span><br><span class="line">            <span class="comment"># 同上</span></span><br><span class="line">            er = (feat_dst * self.attn_r).<span class="built_in">sum</span>(dim=-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将上边计算的特征和注意力值放入节点</span></span><br><span class="line">            graph.srcdata.update(&#123;<span class="string">&quot;ft&quot;</span>: feat_src, <span class="string">&quot;el&quot;</span>: el&#125;)</span><br><span class="line">            graph.dstdata.update(&#123;<span class="string">&quot;er&quot;</span>: er&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算每个边对应的注意力分数，将两个节点的分数加起来</span></span><br><span class="line">            <span class="comment"># 然后把值复制给边，这样每条边都对应一个注意力分数</span></span><br><span class="line">            <span class="comment"># compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.</span></span><br><span class="line">            graph.apply_edges(fn.u_add_v(<span class="string">&quot;el&quot;</span>, <span class="string">&quot;er&quot;</span>, <span class="string">&quot;e&quot;</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 激活函数</span></span><br><span class="line">            e = self.leaky_relu(graph.edata.pop(<span class="string">&quot;e&quot;</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为了使权重归一化，使用edge_softmax</span></span><br><span class="line">            <span class="comment"># compute softmax</span></span><br><span class="line">            <span class="comment"># 最终将每个边对应的注意力分数放入&#x27;a&#x27;</span></span><br><span class="line">            <span class="comment"># 计算结果形状为(num_edges, num_heads, 1)</span></span><br><span class="line">            graph.edata[<span class="string">&quot;a&quot;</span>] = self.attn_drop(edge_softmax(graph, e))</span><br><span class="line">            <span class="comment"># 如果边本身有权重</span></span><br><span class="line">            <span class="keyword">if</span> edge_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 将(num_edges, 1)处理为（num_edges, num_heads, 1）</span></span><br><span class="line">                <span class="comment"># 然后相乘</span></span><br><span class="line">                graph.edata[<span class="string">&quot;a&quot;</span>] = graph.edata[<span class="string">&quot;a&quot;</span>] * edge_weight.tile(</span><br><span class="line">                    <span class="number">1</span>, self._num_heads, <span class="number">1</span></span><br><span class="line">                ).transpose(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 最终的消息传递阶段，对应论文公式（4）或者（5）</span></span><br><span class="line">            <span class="comment"># 消息函数：将节点对应的的特征乘以对应的注意力分数，放入&#x27;m&#x27;，产生消息</span></span><br><span class="line">            <span class="comment"># 聚合函数：聚集节点每个邻边产生的‘m’，求和后放入&#x27;ft&#x27;</span></span><br><span class="line">            <span class="comment"># message passing</span></span><br><span class="line">            graph.update_all(fn.u_mul_e(<span class="string">&quot;ft&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;m&quot;</span>), fn.<span class="built_in">sum</span>(<span class="string">&quot;m&quot;</span>, <span class="string">&quot;ft&quot;</span>))</span><br><span class="line">            rst = graph.dstdata[<span class="string">&quot;ft&quot;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># residual</span></span><br><span class="line">            <span class="comment"># 残差</span></span><br><span class="line">            <span class="keyword">if</span> self.res_fc <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果有残差</span></span><br><span class="line">                <span class="comment"># Use -1 rather than self._num_heads to handle broadcasting</span></span><br><span class="line">                resval = self.res_fc(h_dst).view(</span><br><span class="line">                    *dst_prefix_shape, -<span class="number">1</span>, self._out_feats</span><br><span class="line">                )</span><br><span class="line">                <span class="comment"># 进行残差连接</span></span><br><span class="line">                rst = rst + resval</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 显式的偏置</span></span><br><span class="line">            <span class="comment"># bias</span></span><br><span class="line">            <span class="keyword">if</span> self.has_explicit_bias:</span><br><span class="line">                rst = rst + self.bias.view(</span><br><span class="line">                    *((<span class="number">1</span>,) * <span class="built_in">len</span>(dst_prefix_shape)),</span><br><span class="line">                    self._num_heads,</span><br><span class="line">                    self._out_feats</span><br><span class="line">                )</span><br><span class="line">            <span class="comment"># activation</span></span><br><span class="line">            <span class="keyword">if</span> self.activation:</span><br><span class="line">                rst = self.activation(rst)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> get_attention:</span><br><span class="line">                <span class="keyword">return</span> rst, graph.edata[<span class="string">&quot;a&quot;</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> rst</span><br></pre></td></tr></table></figure>

<h3 id="训练代码"><a href="#训练代码" class="headerlink" title="训练代码"></a>训练代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gatconv <span class="keyword">import</span> GATConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> dgl <span class="keyword">import</span> AddSelfLoop</span><br><span class="line"><span class="keyword">from</span> dgl.data <span class="keyword">import</span> CiteseerGraphDataset, CoraGraphDataset, PubmedGraphDataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GAT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_size, hid_size, out_size, heads</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.gat_layers = nn.ModuleList()</span><br><span class="line">        <span class="comment"># two-layer GAT</span></span><br><span class="line">        self.gat_layers.append(</span><br><span class="line">            GATConv(</span><br><span class="line">                in_size,</span><br><span class="line">                hid_size,</span><br><span class="line">                heads[<span class="number">0</span>],</span><br><span class="line">                feat_drop=<span class="number">0.6</span>,</span><br><span class="line">                attn_drop=<span class="number">0.6</span>,</span><br><span class="line">                activation=F.elu,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        self.gat_layers.append(</span><br><span class="line">            GATConv(</span><br><span class="line">                hid_size * heads[<span class="number">0</span>],</span><br><span class="line">                out_size,</span><br><span class="line">                heads[<span class="number">1</span>],</span><br><span class="line">                feat_drop=<span class="number">0.6</span>,</span><br><span class="line">                attn_drop=<span class="number">0.6</span>,</span><br><span class="line">                activation=<span class="literal">None</span>,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g, inputs</span>):</span><br><span class="line">        h = inputs</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.gat_layers):</span><br><span class="line">            h = layer(g, h)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(self.gat_layers) - <span class="number">1</span>:  <span class="comment"># last layer</span></span><br><span class="line">                h = h.mean(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># other layer(s)</span></span><br><span class="line">                h = h.flatten(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">g, features, labels, mask, model</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        logits = model(g, features)</span><br><span class="line">        logits = logits[mask]</span><br><span class="line">        labels = labels[mask]</span><br><span class="line">        _, indices = torch.<span class="built_in">max</span>(logits, dim=<span class="number">1</span>)</span><br><span class="line">        correct = torch.<span class="built_in">sum</span>(indices == labels)</span><br><span class="line">        <span class="keyword">return</span> correct.item() * <span class="number">1.0</span> / <span class="built_in">len</span>(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">g, features, labels, masks, model, num_epochs</span>):</span><br><span class="line">    <span class="comment"># Define train/val samples, loss function and optimizer</span></span><br><span class="line">    train_mask = masks[<span class="number">0</span>]</span><br><span class="line">    val_mask = masks[<span class="number">1</span>]</span><br><span class="line">    loss_fcn = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">5e-3</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        t0 = time.time()</span><br><span class="line">        model.train()</span><br><span class="line">        logits = model(g, features)</span><br><span class="line">        loss = loss_fcn(logits[train_mask], labels[train_mask])</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        acc = evaluate(g, features, labels, val_mask, model)</span><br><span class="line">        t1 = time.time()</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Epoch &#123;:05d&#125; | Loss &#123;:.4f&#125; | Accuracy &#123;:.4f&#125; | Time &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, loss.item(), acc, t1 - t0</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--dataset&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="string">&quot;cora&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Dataset name (&#x27;cora&#x27;, &#x27;citeseer&#x27;, &#x27;pubmed&#x27;).&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--num_epochs&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">        default=<span class="number">200</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Number of epochs for train.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--num_gpus&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">        default=<span class="number">0</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Number of GPUs used for train and evaluation.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Training with GATConv module.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load and preprocess dataset</span></span><br><span class="line">    transform = (</span><br><span class="line">        AddSelfLoop()</span><br><span class="line">    )  <span class="comment"># by default, it will first remove self-loops to prevent duplication</span></span><br><span class="line">    <span class="keyword">if</span> args.dataset == <span class="string">&quot;cora&quot;</span>:</span><br><span class="line">        data = CoraGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">elif</span> args.dataset == <span class="string">&quot;citeseer&quot;</span>:</span><br><span class="line">        data = CiteseerGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">elif</span> args.dataset == <span class="string">&quot;pubmed&quot;</span>:</span><br><span class="line">        data = PubmedGraphDataset(transform=transform)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Unknown dataset: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(args.dataset))</span><br><span class="line">    g = data[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> args.num_gpus &gt; <span class="number">0</span> <span class="keyword">and</span> torch.cuda.is_available():</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    g = g.<span class="built_in">int</span>().to(device)</span><br><span class="line">    features = g.ndata[<span class="string">&quot;feat&quot;</span>]</span><br><span class="line">    labels = g.ndata[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">    masks = g.ndata[<span class="string">&quot;train_mask&quot;</span>], g.ndata[<span class="string">&quot;val_mask&quot;</span>], g.ndata[<span class="string">&quot;test_mask&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create GAT model</span></span><br><span class="line">    in_size = features.shape[<span class="number">1</span>]</span><br><span class="line">    out_size = data.num_classes</span><br><span class="line">    model = GAT(in_size, <span class="number">8</span>, out_size, heads=[<span class="number">8</span>, <span class="number">1</span>]).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training...&quot;</span>)</span><br><span class="line">    train(g, features, labels, masks, model, args.num_epochs)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Testing...&quot;</span>)</span><br><span class="line">    acc = evaluate(g, features, labels, masks[<span class="number">2</span>], model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test accuracy &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(acc))</span><br></pre></td></tr></table></figure>

<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/dmlc/dgl/blob/master/examples/core/gat/train.py">dgl&#x2F;examples&#x2F;core&#x2F;gat&#x2F;train.py</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/gatconv.py">dgl&#x2F;python&#x2F;dgl&#x2F;nn&#x2F;pytorch&#x2F;conv&#x2F;gatconv.py</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/GAT%E5%AD%A6%E4%B9%A0/" data-id="clnn4xjdm0000w0u27hiwgc9n" data-title="GAT学习" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-When Do GNNs Work" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/When%20Do%20GNNs%20Work/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T12:06:58.516Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/When%20Do%20GNNs%20Work/">When Do GNNs Work</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="When-Do-GNNs-Work"><a href="#When-Do-GNNs-Work" class="headerlink" title="When Do GNNs Work"></a>When Do GNNs Work</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li>现存的多种GNN模型中，包含的一个关键的部分是<strong>邻居聚合</strong>，其中每个节点的嵌入向量是通过参考它的邻居节点更新的。</li>
<li>本文旨在通过以下问题为这种机制提供一种更好的理解<ul>
<li>邻居聚合是否总是必要和有用的？答案是否定的，在以下两个情况中，邻居聚合是无益的<ol>
<li>当一个节点的邻居节点<strong>高度不相似</strong></li>
<li>当一个节点的嵌入<strong>已经</strong>和它的邻居节点相似时</li>
</ol>
</li>
</ul>
</li>
<li>本文提出了一种新的<strong>度量方式</strong>，定量的衡量这两种情况并将他们融合进一个自适应层。</li>
</ul>
<h2 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h2><ul>
<li><p>已经有研究证明，过高的聚合程度将来自不同集群的节点混合在一起，并将这种现象称为<strong>“over-smoothing”</strong></p>
</li>
<li><p>但是，他们没有都没有从局部考虑不同的聚集度。事实上，如果我们允许聚合度在节点之间变化，GNN的性能可以显著提高。</p>
</li>
<li><p>那么，现在的问题是，<strong>如何控制单个节点的聚合程度？</strong></p>
</li>
<li><p>为了解决以上问题，本文分析出两种聚合无用的情况</p>
<ol>
<li>如果中心节点的邻居的学习到的特征&#x2F;标签<strong>不一致</strong>（<strong>高熵</strong>），则进一步聚合可能会损害性能;</li>
<li>当中心节点的学习到的特征&#x2F;标签<strong>与其邻居几乎相同时</strong>，不需要进一步聚合。</li>
</ol>
</li>
<li><p>本文设计了一个<strong>Adaptive-layer</strong>，在训练过程中，检查每个节点的邻居节点的学习到的标签，并通过作者设计的metric，衡量聚合该邻居是否有用，并只允许有益的节点进行聚合。</p>
</li>
</ul>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ol>
<li>从局部的视角分析了邻域聚合的作用</li>
<li>提出了两个直观且有规则的度量方法定量的描述了邻域聚合没有帮助的两种情况</li>
<li>将度量标准融合进一种新的<strong>自适应层</strong>的设计中</li>
</ol>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>GNN更新函数一般包含以下操作：非线性，线性映射，邻域聚合</p>
<p>过多的聚合会导致过平滑的现象。避免性能恶化的关键在于防止社团间（具有不同类别标签的节点）之间的混合。由此提出了本文的第一种度量：<strong>neighborhood entropy</strong></p>
<p>此外，不同的节点最佳的<strong>聚合度</strong>是不一样的，因此本文提出第二种度量：<strong>center-neighbor similarity</strong></p>
<h3 id="Neighborhood-Entropy"><a href="#Neighborhood-Entropy" class="headerlink" title="Neighborhood Entropy"></a>Neighborhood Entropy</h3><p>邻域聚合利用了网络中的同质性效应，即连接的节点应该是相似的。</p>
<p>进一步，节点的邻居们应该彼此相似。当邻居不同意时，我们将此视为假设可能不成立并且聚合信息可能是噪声的警告。</p>
<p>为了衡量一个特定节点的邻域的差异，计算邻域熵如下：</p>
<p>$$Score_{etp}(u)&#x3D;-\int_{\mathbb{X}}f^{\mathcal{N}(u)}(x)\cdot\log(f^{\mathcal{N}(u)}{(x)})dx, \quad(5)$$</p>
<ul>
<li>$\mathbb{X}$ 代表特征空间</li>
<li>$f^{\mathcal{N}(u)}$ 为节点 $u$ 的邻居的特征的 <strong>概率密度函数</strong>（Probability Density Function）(PDF)</li>
</ul>
<p>然而，由于PDF是每个邻居节点的<strong>狄拉克函数</strong>在高维空间的和，因此计算该微分熵是不可行的，并且不是非常有用的。</p>
<p>我们使用预测的标签来计算节点 $u$ 的邻居的标签分布并计算其离散熵：</p>
<p>$$\begin{aligned}<br>Score_{etp}(u)&amp; &#x3D;-\sum_{c\in C}P_{c}(u)\log(P_{c}(u)),  \<br>&amp;&amp;\text{(6)} \<br>P_{c}(u)&amp; &#x3D;\frac{|{v\in\mathcal{N}(u)|y_v&#x3D;c}|}{|\mathcal{N}(u)|},<br>\end{aligned}$$</p>
<ul>
<li>$C$ 是所有标签的类别集合</li>
<li>$Score_{etp}(u)$ 更大的话，节点 $u$ 的邻居节点的区别就更大</li>
</ul>
<h3 id="Center-Neighbor-Similarity"><a href="#Center-Neighbor-Similarity" class="headerlink" title="Center-Neighbor Similarity"></a>Center-Neighbor Similarity</h3><p>当节点的特征与邻居节点的特征已经足够相似时，邻域聚合操作可能是多余的。</p>
<p>本文使用 <strong>pointwise mutual information(PMI)</strong> 描述中心节点和他的邻居们的相似性。</p>
<p>$$PMI(u;\mathcal{N}(u))&#x3D;\frac{P(\mathcal{N}(u)|u)}{P(\mathcal{N}(u))}$$</p>
<p>由于我们没有邻居特征的概率分布的先验知识，我们假设它遵循均匀分布，这使得${P(\mathcal{N}(u))}$是常数。然后，相似性定义如下：</p>
<p>$$Score_{sim}(u)&#x3D;P(\mathcal{N}(u)|u)&#x3D;\frac{1}{|\mathcal{N}(u)|}\sum_{v\in\mathcal{N}(u)}\frac{f_u^Tf_v}{\sum_{k\in V}f_u^Tf_k},\quad(7)$$</p>
<ul>
<li>$u$ 为中心节点</li>
<li>$\mathcal{N}(u)$ 是节点 $u$ 的邻居集。</li>
<li>$f(u)$ 是节点 $u$ 的特征，可能是输入特征，学习到的特征，或者是节点预测的标签。</li>
</ul>
<p>相似的，我们可以使用独热编码预测标签以计算该指标</p>
<p>$$Score_{sim}(u)&#x3D;\frac{|{v\in\mathcal{N}(u)|y_{v}&#x3D;y_{u}}|}{|\mathcal{N}(u)|\cdot|{v\in V|y_{v}&#x3D;y_{u}}|}.\quad(8)$$</p>
<ul>
<li>$Score_{sim}(u)$ 越大，节点$u$与他的邻居越相似</li>
</ul>
<p>在这种情况下，我们可以比较邻域聚合之前和之后的性能，并表明预测结果几乎相同，如以下定理1中正式推导的。</p>
<h3 id="Theorem-1"><a href="#Theorem-1" class="headerlink" title="Theorem 1"></a>Theorem 1</h3><p>假设我们使用每个标签的预测概率分布 $h_u$ 来计算$Score_{sim}$。</p>
<p>如果对所有的 $u \in V$ , 有 $Score_{sim}(u)\geq\epsilon $ 。则邻域聚合前后的 2-norm 损失之差 $\Delta\mathcal{L}\leq\sqrt{2(1-\frac{\epsilon|V|}{|C|})}$</p>
<p><strong>证明：</strong></p>
<ul>
<li>$f_u$ 为节点 $u$ 的初始特征</li>
<li>$l_u$ 是节点 $u$ 的真是独热标签向量</li>
<li>$h_u$ 是节点 $u$ 初始的预测标签分布</li>
</ul>
<p>聚合后的预测标签分布为：$\hat{h_{u}}&#x3D;\frac{1}{|\mathcal{N}(u)+1|}\sum_{v\in\mathcal{N}(u)\bigcup{u}}h_{v}$</p>
<p>$$\because\langle h_{u},\hat{h}<em>{u}\rangle\geq\langle h</em>{u},\frac{\sum_{v\in\mathcal{N}(u)}h_{v}}{|\mathcal{N}(u)|}\rangle\geq\epsilon\sum_{u\in V}\langle h_{u},h_{v}\rangle,\forall u\in V$$</p>
<blockquote>
<p>因为 $\hat{h}_{u}$ 包含了节点 $u$ 本身的信息, 因此$h_u$与$\hat{h}_u$的内积要比后边的大</p>
</blockquote>
<blockquote>
<p>由于 $\epsilon \leq \frac{1}{|\mathcal{N}_{u}|}$ , 后半个不等式成立</p>
</blockquote>
<p>$$\therefore\sum_{u\in V}\langle h_{u},\hat{h_{u}}\rangle\geq\epsilon\langle\sum_{u\in V}h_{u},\sum_{u\in V}h_{u}\rangle\geq\epsilon\frac{|V|^{2}}{|C|}$$</p>
<blockquote>
<p>$\langle\sum_{u\in V}h_{u},\sum_{u\in V}h_{u}\rangle \geq |V|^2\langle{h_{avg}},{h_{avg}}\rangle \geq \frac{|V|^2}{|C|}$</p>
</blockquote>
<p>$$\begin{aligned}<br>&amp;\Delta\mathcal{L}&#x3D;\frac{1}{|V|}\sum_{u\in V}(||h_{u}-l_{u}||-||\hat{h_{u}}-l_{u}||) \<br>&amp;\leq\frac{1}{|V|}\sum_{u\in V}||h_u-\hat{h_u}||\leq\frac{\sqrt{|V|}}{|V|}\sqrt{\sum_{u\in V}||h_u-\hat{h_u}||^2} \<br>&amp;&#x3D;\frac{\sqrt{|V|}}{|V|}\sqrt{\sum_{u\in V}||h_u||^2+\sum_{u\in V}||\hat{h_u}||^2-2\sum_{u\in V}\langle h_u,\hat{h_u}\rangle} \<br>&amp;\leq\frac{\sqrt{|V|}}{|V|}\sqrt{2|V|-2\frac{\epsilon|V|^2}{|C|}}&#x3D;\sqrt{2(1-\frac{\epsilon|V|}{|C|})}.<br>\end{aligned}$$</p>
<blockquote>
<p>第一步推导根据范数的三角不等式</p>
<p>$\begin{align}&amp;\frac{1}{|V|}\sum_{u\in V}(||h_{u}-l_{u}||-||\hat{h_{u}}-l_{u}||)\ &amp;\leq \frac{1}{|V|}\sum_{u\in V}(||h_u|| + ||l_u|| - ||\hat{h}<em>u|| - ||l_u||) \quad (两边之和大于第三边) \ &amp;&#x3D;\frac{1}{|V|}\sum</em>{u\in V}(||h_u|| - ||\hat{h}<em>u||)\ &amp;\leq \frac{1}{|V|}\sum</em>{u\in V}(||h_u - \hat{h}_u||) \quad(两边之差小于第三边) \end{align}$</p>
</blockquote>
<blockquote>
<p>$\begin{align} &amp;||h_u - \hat{h}_u||^2 \&amp;&#x3D; ||h_u - \hat{h}_u|| \cdot ||h_u - \hat{h}<em>u|| \&amp; &#x3D; ||h_u||^2 + ||\hat{h}<em>u||^2 - 2 \langle h</em>{u},\hat{h}</em>{u}\rangle \end{align}$ </p>
</blockquote>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>经过上述分析,本文提出了一个 <strong>自适应层</strong>, 允许节点在每一轮邻域聚合的时候做出独立的决定。最终，每个节点可能会经历不同的 <strong>聚合度</strong>。</p>
<p>具体来说，在每一层中应用门控函数，控制邻域信息的影响。它的值由$Score_{sim}$ 和 $Score_{etp}$ 决定。</p>
<p>与SGC的精神相似，本文去除了所有的非线性层。</p>
<p>模型结构如下：</p>
<p>$$\begin{aligned}<br>h_{u}^{l+1}&amp; &#x3D;h_{u}^{l}+z_{l,u}Agg({h_{v}^{l}|v\in\mathcal{N}(u)}),\quad(1&lt;l&lt;L)  \<br>h_{u}^{1}&amp; &#x3D;W_{h}Agg({x_{v}|v\in\mathcal{N}(u)}),  \<br>y_{u}&amp; &#x3D;\mathrm{softmax}(W_{y}h_{u}^{L}),<br>\end{aligned} \quad(9)$$</p>
<ul>
<li>$z_{l,u}$ 是控制邻域聚合的随机变量</li>
<li>更新函数类似于残差层，原因如下：<ol>
<li>残差层可以让模型堆叠更多的层</li>
<li>可以用同样的映射矩阵 $W_y$ 将隐藏状态 $h_u^l$ 映射到标签</li>
</ol>
</li>
</ul>
<p>使用以下公式计算门 $z_{l,u}$</p>
<p>$$z_{l,u}&#x3D;\sigma(\tau_{1}-\mathrm{Norm}(Score_{sim}(l,u)))  \cdot\sigma(\tau_{2}-\mathrm{Norm}(Score_{etp}(l,u))).<br> \quad(10)$$</p>
<ul>
<li><p>激活函数 $\sigma$ 将 $z$ 值压缩到（0,1)</p>
</li>
<li><p>当$Score_{sim}$ 和 $Score_{etp}$ 都很大时，$z$ 会是一个接近0的数</p>
</li>
<li><p>$Norm$ 是批量归一化操作，用于重新缩放分数，以便它们在各层之间具有可比性。</p>
</li>
</ul>
<p>为了简化，我们使用独热预测标签计算$Score_{sim}$ 和 $Score_{etp}$。 因为我们没有实际类大小的先验知识，我们假设所有的标签都具有相同的类大小。因此，标签类别大小项是常数，并且从计算中省略。</p>
<p>为了与基于注意力的方法比较，我们可以拓展模型以处理邻居的注意力权重。</p>
<p>$$\begin{aligned}<br>Score_{sim}^{att}(l,u)&amp; &#x3D;\sum_{v\in\mathcal{N}(u),y_{v}^{l}&#x3D;y_{u}^{l}}a_{u,v}^{l},  \<br>Score_{etp}^{att}(l,u)&amp; \begin{aligned}&#x3D;-\sum_{y\in Y},P_{y}^{att}(l,u)\log(P_{y}^{att}(l,u))\quad(11)\end{aligned}  \<br>P_{y}^{att}(l,u)&amp; &#x3D;\sum_{v\in\mathcal{N}(u),y_{v}^{l}&#x3D;y}a_{u,v}^{l},<br>\end{aligned}$$</p>
<ul>
<li>$a_{u,v}^{l}$ 是节点 $u$ 对节点 $v$ 在 $l$ 层的注意力系数</li>
</ul>
<p>我们还可以通过为每个注意力头部计算不同的 $z$ 来将我们的度量扩展到多头注意力，表示为$z^k_{l,u}$：</p>
<p>$$\begin{aligned}<br>h_{u}^{l+1}&amp; &#x3D;\mathop|\limits^K_{k&#x3D;1}h_{u}^{l}+z_{l,u}^{k}Agg({h_{v}^{l}|v\in\mathcal{N}(u)})\quad(l&lt;L-1),  \<br>h_{u}^{L}&amp; &#x3D;\frac{1}{K}\sum_{k&#x3D;1}^{K}(h_{u}^{L-1}+z_{L-1,u}^{k}Agg({h_{v}^{L-1}|v\in\mathcal{N}(u)})).  \<br>&amp;&amp;\text{2)}<br>\end{aligned}$$</p>
<p><strong>ALaGCN</strong> <strong>model</strong></p>
<p><img src="https://img-blog.csdnimg.cn/a9471d2d33a840d1ba0df15dc2393536.png" alt="在这里插入图片描述"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/When%20Do%20GNNs%20Work/" data-id="clnn4xje20007w0u22sxjeghu" data-title="When Do GNNs Work" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/12/hello-world/" class="article-date">
  <time class="dt-published" datetime="2023-10-12T04:01:09.208Z" itemprop="datePublished">2023-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/12/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/12/hello-world/" data-id="clnn4xje70008w0u2fl917xdm" data-title="Hello World" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/10/12/SUGRL/">SUGRL</a>
          </li>
        
          <li>
            <a href="/2023/10/12/SGNN/">SGNN学习</a>
          </li>
        
          <li>
            <a href="/2023/10/12/PathNet/">PathNet学习</a>
          </li>
        
          <li>
            <a href="/2023/10/12/HAN%E5%AD%A6%E4%B9%A0/">HAN学习</a>
          </li>
        
          <li>
            <a href="/2023/10/12/GraphSAGE%E5%AD%A6%E4%B9%A0/">GraphSAGE学习</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Wan Li<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>